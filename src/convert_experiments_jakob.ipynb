{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 16:43:01.403972: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-16 16:43:02.352488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 16:43:02.352591: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 16:43:02.352599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.serialization import from_bytes\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from resnet20 import BLOCKS_PER_GROUP, ResNet\n",
    "import cifar100_resnet20_train  # import runa code that is necessary to run the model\n",
    "from flax.traverse_util import flatten_dict\n",
    "import numpy as np\n",
    "from utils import flatten_params, unflatten_params\n",
    "from weight_matching import resnet20_permutation_spec\n",
    "from resnet20_torch import resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"],\n",
    "                   num_classes=512,\n",
    "                   width_multiplier=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key1 , key2 = random.split(random.PRNGKey(0))\n",
    "model_params = model.init(key2, jnp.zeros((1, 32, 32, 3)))\n",
    "# from_bytes(model.init(random.PRNGKey(0), jnp.zeros((1, 32, 32, 3)))[\"params\"], fh.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_model(filepath):\n",
    "#     with open(filepath, \"rb\") as fh:\n",
    "#         return from_bytes(\n",
    "#             model.init(random.PRNGKey(0), jnp.zeros((1, 32, 32, 3)))[\"params\"], fh.read())\n",
    "# model = load_model(\"../v7\")\n",
    "params = flatten_params(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = resnet20_permutation_spec().axes_to_perm\n",
    "# this lines up the keys of corresponding dictionaries, at least it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_save_path = \"/srv/share/jbjorner3/checkpoints/REPAIR/resnet20x4_CIFAR50_clses[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49].pth.tar\"\n",
    "model2_save_path = \"/srv/share/jbjorner3/checkpoints/REPAIR/resnet20x4_CIFAR50_clses[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99].pth.tar\"\n",
    "# sd = torch.load('model_1.th')[\"state_dict\"]\n",
    "# sd = {k: v.cpu() for k, v in sd.items()}\n",
    "# len(sd.keys())\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model1 = resnet20(w=4).to(DEVICE)\n",
    "sd = torch.load(model1_save_path, map_location=torch.device(DEVICE))\n",
    "sd = {k: v.cpu() for k, v in sd.items()}\n",
    "model1.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sd.keys()).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_linen(torch_params, get_flax_keys):\n",
    "    \"\"\"Convert PyTorch parameters to Linen nested dictionaries\"\"\"\n",
    "\n",
    "    def add_to_params(params_dict, nested_keys, param, is_conv=False):\n",
    "        if len(nested_keys) == 1:\n",
    "            key, = nested_keys\n",
    "            try:\n",
    "                params_dict[key] = np.transpose(param, (2, 3, 1, 0)) if is_conv else np.transpose(param)\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "        else:\n",
    "            assert len(nested_keys) > 1\n",
    "            first_key = nested_keys[0]\n",
    "            if first_key not in params_dict:\n",
    "                params_dict[first_key] = {}\n",
    "            add_to_params(params_dict[first_key], nested_keys[1:], param, ('conv' in first_key and \\\n",
    "                                                                         nested_keys[-1] != 'bias'))\n",
    "\n",
    "    flax_params = {'params': {}, 'batch_stats': {}}\n",
    "    for key, tensor in torch_params.items():\n",
    "        flax_keys = get_flax_keys(key.split('.'))\n",
    "        if flax_keys[-1] is not None:\n",
    "            if flax_keys[-1] in ('mean', 'var'):\n",
    "                add_to_params(flax_params['batch_stats'], flax_keys, tensor.detach().numpy())\n",
    "            else:\n",
    "                add_to_params(flax_params['params'], flax_keys, tensor.detach().numpy())\n",
    "\n",
    "    return flax_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([64, 3, 3, 3]), 'conv1.weight'),\n",
       " (torch.Size([64]), 'bn1.weight'),\n",
       " (torch.Size([64]), 'bn1.bias'),\n",
       " (torch.Size([64]), 'bn1.running_mean'),\n",
       " (torch.Size([64]), 'bn1.running_var'),\n",
       " (torch.Size([]), 'bn1.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.0.conv1.weight'),\n",
       " (torch.Size([64]), 'layer1.0.bn1.weight'),\n",
       " (torch.Size([64]), 'layer1.0.bn1.bias'),\n",
       " (torch.Size([64]), 'layer1.0.bn1.running_mean'),\n",
       " (torch.Size([64]), 'layer1.0.bn1.running_var'),\n",
       " (torch.Size([]), 'layer1.0.bn1.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.0.conv2.weight'),\n",
       " (torch.Size([64]), 'layer1.0.bn2.weight'),\n",
       " (torch.Size([64]), 'layer1.0.bn2.bias'),\n",
       " (torch.Size([64]), 'layer1.0.bn2.running_mean'),\n",
       " (torch.Size([64]), 'layer1.0.bn2.running_var'),\n",
       " (torch.Size([]), 'layer1.0.bn2.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.1.conv1.weight'),\n",
       " (torch.Size([64]), 'layer1.1.bn1.weight'),\n",
       " (torch.Size([64]), 'layer1.1.bn1.bias'),\n",
       " (torch.Size([64]), 'layer1.1.bn1.running_mean'),\n",
       " (torch.Size([64]), 'layer1.1.bn1.running_var'),\n",
       " (torch.Size([]), 'layer1.1.bn1.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.1.conv2.weight'),\n",
       " (torch.Size([64]), 'layer1.1.bn2.weight'),\n",
       " (torch.Size([64]), 'layer1.1.bn2.bias'),\n",
       " (torch.Size([64]), 'layer1.1.bn2.running_mean'),\n",
       " (torch.Size([64]), 'layer1.1.bn2.running_var'),\n",
       " (torch.Size([]), 'layer1.1.bn2.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.2.conv1.weight'),\n",
       " (torch.Size([64]), 'layer1.2.bn1.weight'),\n",
       " (torch.Size([64]), 'layer1.2.bn1.bias'),\n",
       " (torch.Size([64]), 'layer1.2.bn1.running_mean'),\n",
       " (torch.Size([64]), 'layer1.2.bn1.running_var'),\n",
       " (torch.Size([]), 'layer1.2.bn1.num_batches_tracked'),\n",
       " (torch.Size([64, 64, 3, 3]), 'layer1.2.conv2.weight'),\n",
       " (torch.Size([64]), 'layer1.2.bn2.weight'),\n",
       " (torch.Size([64]), 'layer1.2.bn2.bias'),\n",
       " (torch.Size([64]), 'layer1.2.bn2.running_mean'),\n",
       " (torch.Size([64]), 'layer1.2.bn2.running_var'),\n",
       " (torch.Size([]), 'layer1.2.bn2.num_batches_tracked'),\n",
       " (torch.Size([128, 64, 3, 3]), 'layer2.0.conv1.weight'),\n",
       " (torch.Size([128]), 'layer2.0.bn1.weight'),\n",
       " (torch.Size([128]), 'layer2.0.bn1.bias'),\n",
       " (torch.Size([128]), 'layer2.0.bn1.running_mean'),\n",
       " (torch.Size([128]), 'layer2.0.bn1.running_var'),\n",
       " (torch.Size([]), 'layer2.0.bn1.num_batches_tracked'),\n",
       " (torch.Size([128, 128, 3, 3]), 'layer2.0.conv2.weight'),\n",
       " (torch.Size([128]), 'layer2.0.bn2.weight'),\n",
       " (torch.Size([128]), 'layer2.0.bn2.bias'),\n",
       " (torch.Size([128]), 'layer2.0.bn2.running_mean'),\n",
       " (torch.Size([128]), 'layer2.0.bn2.running_var'),\n",
       " (torch.Size([]), 'layer2.0.bn2.num_batches_tracked'),\n",
       " (torch.Size([128, 64, 3, 3]), 'layer2.0.shortcut.0.weight'),\n",
       " (torch.Size([128]), 'layer2.0.shortcut.1.weight'),\n",
       " (torch.Size([128]), 'layer2.0.shortcut.1.bias'),\n",
       " (torch.Size([128]), 'layer2.0.shortcut.1.running_mean'),\n",
       " (torch.Size([128]), 'layer2.0.shortcut.1.running_var'),\n",
       " (torch.Size([]), 'layer2.0.shortcut.1.num_batches_tracked'),\n",
       " (torch.Size([128, 128, 3, 3]), 'layer2.1.conv1.weight'),\n",
       " (torch.Size([128]), 'layer2.1.bn1.weight'),\n",
       " (torch.Size([128]), 'layer2.1.bn1.bias'),\n",
       " (torch.Size([128]), 'layer2.1.bn1.running_mean'),\n",
       " (torch.Size([128]), 'layer2.1.bn1.running_var'),\n",
       " (torch.Size([]), 'layer2.1.bn1.num_batches_tracked'),\n",
       " (torch.Size([128, 128, 3, 3]), 'layer2.1.conv2.weight'),\n",
       " (torch.Size([128]), 'layer2.1.bn2.weight'),\n",
       " (torch.Size([128]), 'layer2.1.bn2.bias'),\n",
       " (torch.Size([128]), 'layer2.1.bn2.running_mean'),\n",
       " (torch.Size([128]), 'layer2.1.bn2.running_var'),\n",
       " (torch.Size([]), 'layer2.1.bn2.num_batches_tracked'),\n",
       " (torch.Size([128, 128, 3, 3]), 'layer2.2.conv1.weight'),\n",
       " (torch.Size([128]), 'layer2.2.bn1.weight'),\n",
       " (torch.Size([128]), 'layer2.2.bn1.bias'),\n",
       " (torch.Size([128]), 'layer2.2.bn1.running_mean'),\n",
       " (torch.Size([128]), 'layer2.2.bn1.running_var'),\n",
       " (torch.Size([]), 'layer2.2.bn1.num_batches_tracked'),\n",
       " (torch.Size([128, 128, 3, 3]), 'layer2.2.conv2.weight'),\n",
       " (torch.Size([128]), 'layer2.2.bn2.weight'),\n",
       " (torch.Size([128]), 'layer2.2.bn2.bias'),\n",
       " (torch.Size([128]), 'layer2.2.bn2.running_mean'),\n",
       " (torch.Size([128]), 'layer2.2.bn2.running_var'),\n",
       " (torch.Size([]), 'layer2.2.bn2.num_batches_tracked'),\n",
       " (torch.Size([256, 128, 3, 3]), 'layer3.0.conv1.weight'),\n",
       " (torch.Size([256]), 'layer3.0.bn1.weight'),\n",
       " (torch.Size([256]), 'layer3.0.bn1.bias'),\n",
       " (torch.Size([256]), 'layer3.0.bn1.running_mean'),\n",
       " (torch.Size([256]), 'layer3.0.bn1.running_var'),\n",
       " (torch.Size([]), 'layer3.0.bn1.num_batches_tracked'),\n",
       " (torch.Size([256, 256, 3, 3]), 'layer3.0.conv2.weight'),\n",
       " (torch.Size([256]), 'layer3.0.bn2.weight'),\n",
       " (torch.Size([256]), 'layer3.0.bn2.bias'),\n",
       " (torch.Size([256]), 'layer3.0.bn2.running_mean'),\n",
       " (torch.Size([256]), 'layer3.0.bn2.running_var'),\n",
       " (torch.Size([]), 'layer3.0.bn2.num_batches_tracked'),\n",
       " (torch.Size([256, 128, 3, 3]), 'layer3.0.shortcut.0.weight'),\n",
       " (torch.Size([256]), 'layer3.0.shortcut.1.weight'),\n",
       " (torch.Size([256]), 'layer3.0.shortcut.1.bias'),\n",
       " (torch.Size([256]), 'layer3.0.shortcut.1.running_mean'),\n",
       " (torch.Size([256]), 'layer3.0.shortcut.1.running_var'),\n",
       " (torch.Size([]), 'layer3.0.shortcut.1.num_batches_tracked'),\n",
       " (torch.Size([256, 256, 3, 3]), 'layer3.1.conv1.weight'),\n",
       " (torch.Size([256]), 'layer3.1.bn1.weight'),\n",
       " (torch.Size([256]), 'layer3.1.bn1.bias'),\n",
       " (torch.Size([256]), 'layer3.1.bn1.running_mean'),\n",
       " (torch.Size([256]), 'layer3.1.bn1.running_var'),\n",
       " (torch.Size([]), 'layer3.1.bn1.num_batches_tracked'),\n",
       " (torch.Size([256, 256, 3, 3]), 'layer3.1.conv2.weight'),\n",
       " (torch.Size([256]), 'layer3.1.bn2.weight'),\n",
       " (torch.Size([256]), 'layer3.1.bn2.bias'),\n",
       " (torch.Size([256]), 'layer3.1.bn2.running_mean'),\n",
       " (torch.Size([256]), 'layer3.1.bn2.running_var'),\n",
       " (torch.Size([]), 'layer3.1.bn2.num_batches_tracked'),\n",
       " (torch.Size([256, 256, 3, 3]), 'layer3.2.conv1.weight'),\n",
       " (torch.Size([256]), 'layer3.2.bn1.weight'),\n",
       " (torch.Size([256]), 'layer3.2.bn1.bias'),\n",
       " (torch.Size([256]), 'layer3.2.bn1.running_mean'),\n",
       " (torch.Size([256]), 'layer3.2.bn1.running_var'),\n",
       " (torch.Size([]), 'layer3.2.bn1.num_batches_tracked'),\n",
       " (torch.Size([256, 256, 3, 3]), 'layer3.2.conv2.weight'),\n",
       " (torch.Size([256]), 'layer3.2.bn2.weight'),\n",
       " (torch.Size([256]), 'layer3.2.bn2.bias'),\n",
       " (torch.Size([256]), 'layer3.2.bn2.running_mean'),\n",
       " (torch.Size([256]), 'layer3.2.bn2.running_var'),\n",
       " (torch.Size([]), 'layer3.2.bn2.num_batches_tracked'),\n",
       " (torch.Size([512, 256]), 'linear.weight'),\n",
       " (torch.Size([512]), 'linear.bias')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((v.shape, k) for k, v in sd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 65)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sd), len(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1.weight': 'conv1/kernel',\n",
       " 'bn1.weight': 'norm1/scale',\n",
       " 'bn1.bias': 'norm1/bias',\n",
       " 'bn1.running_mean': 'blockgroups_0/blocks_0/conv1/kernel',\n",
       " 'bn1.running_var': 'blockgroups_0/blocks_0/norm1/scale',\n",
       " 'bn1.num_batches_tracked': 'blockgroups_0/blocks_0/norm1/bias',\n",
       " 'layer1.0.conv1.weight': 'blockgroups_0/blocks_0/conv2/kernel',\n",
       " 'layer1.0.bn1.weight': 'blockgroups_0/blocks_0/norm2/scale',\n",
       " 'layer1.0.bn1.bias': 'blockgroups_0/blocks_0/norm2/bias',\n",
       " 'layer1.0.bn1.running_mean': 'blockgroups_0/blocks_1/conv1/kernel',\n",
       " 'layer1.0.bn1.running_var': 'blockgroups_0/blocks_1/norm1/scale',\n",
       " 'layer1.0.bn1.num_batches_tracked': 'blockgroups_0/blocks_1/norm1/bias',\n",
       " 'layer1.0.conv2.weight': 'blockgroups_0/blocks_1/conv2/kernel',\n",
       " 'layer1.0.bn2.weight': 'blockgroups_0/blocks_1/norm2/scale',\n",
       " 'layer1.0.bn2.bias': 'blockgroups_0/blocks_1/norm2/bias',\n",
       " 'layer1.0.bn2.running_mean': 'blockgroups_0/blocks_2/conv1/kernel',\n",
       " 'layer1.0.bn2.running_var': 'blockgroups_0/blocks_2/norm1/scale',\n",
       " 'layer1.0.bn2.num_batches_tracked': 'blockgroups_0/blocks_2/norm1/bias',\n",
       " 'layer1.1.conv1.weight': 'blockgroups_0/blocks_2/conv2/kernel',\n",
       " 'layer1.1.bn1.weight': 'blockgroups_0/blocks_2/norm2/scale',\n",
       " 'layer1.1.bn1.bias': 'blockgroups_0/blocks_2/norm2/bias',\n",
       " 'layer1.1.bn1.running_mean': 'blockgroups_1/blocks_0/conv1/kernel',\n",
       " 'layer1.1.bn1.running_var': 'blockgroups_1/blocks_0/norm1/scale',\n",
       " 'layer1.1.bn1.num_batches_tracked': 'blockgroups_1/blocks_0/norm1/bias',\n",
       " 'layer1.1.conv2.weight': 'blockgroups_1/blocks_0/conv2/kernel',\n",
       " 'layer1.1.bn2.weight': 'blockgroups_1/blocks_0/norm2/scale',\n",
       " 'layer1.1.bn2.bias': 'blockgroups_1/blocks_0/norm2/bias',\n",
       " 'layer1.1.bn2.running_mean': 'blockgroups_1/blocks_0/shortcut/layers_0/kernel',\n",
       " 'layer1.1.bn2.running_var': 'blockgroups_1/blocks_0/shortcut/layers_1/scale',\n",
       " 'layer1.1.bn2.num_batches_tracked': 'blockgroups_1/blocks_0/shortcut/layers_1/bias',\n",
       " 'layer1.2.conv1.weight': 'blockgroups_1/blocks_1/conv1/kernel',\n",
       " 'layer1.2.bn1.weight': 'blockgroups_1/blocks_1/norm1/scale',\n",
       " 'layer1.2.bn1.bias': 'blockgroups_1/blocks_1/norm1/bias',\n",
       " 'layer1.2.bn1.running_mean': 'blockgroups_1/blocks_1/conv2/kernel',\n",
       " 'layer1.2.bn1.running_var': 'blockgroups_1/blocks_1/norm2/scale',\n",
       " 'layer1.2.bn1.num_batches_tracked': 'blockgroups_1/blocks_1/norm2/bias',\n",
       " 'layer1.2.conv2.weight': 'blockgroups_1/blocks_2/conv1/kernel',\n",
       " 'layer1.2.bn2.weight': 'blockgroups_1/blocks_2/norm1/scale',\n",
       " 'layer1.2.bn2.bias': 'blockgroups_1/blocks_2/norm1/bias',\n",
       " 'layer1.2.bn2.running_mean': 'blockgroups_1/blocks_2/conv2/kernel',\n",
       " 'layer1.2.bn2.running_var': 'blockgroups_1/blocks_2/norm2/scale',\n",
       " 'layer1.2.bn2.num_batches_tracked': 'blockgroups_1/blocks_2/norm2/bias',\n",
       " 'layer2.0.conv1.weight': 'blockgroups_2/blocks_0/conv1/kernel',\n",
       " 'layer2.0.bn1.weight': 'blockgroups_2/blocks_0/norm1/scale',\n",
       " 'layer2.0.bn1.bias': 'blockgroups_2/blocks_0/norm1/bias',\n",
       " 'layer2.0.bn1.running_mean': 'blockgroups_2/blocks_0/conv2/kernel',\n",
       " 'layer2.0.bn1.running_var': 'blockgroups_2/blocks_0/norm2/scale',\n",
       " 'layer2.0.bn1.num_batches_tracked': 'blockgroups_2/blocks_0/norm2/bias',\n",
       " 'layer2.0.conv2.weight': 'blockgroups_2/blocks_0/shortcut/layers_0/kernel',\n",
       " 'layer2.0.bn2.weight': 'blockgroups_2/blocks_0/shortcut/layers_1/scale',\n",
       " 'layer2.0.bn2.bias': 'blockgroups_2/blocks_0/shortcut/layers_1/bias',\n",
       " 'layer2.0.bn2.running_mean': 'blockgroups_2/blocks_1/conv1/kernel',\n",
       " 'layer2.0.bn2.running_var': 'blockgroups_2/blocks_1/norm1/scale',\n",
       " 'layer2.0.bn2.num_batches_tracked': 'blockgroups_2/blocks_1/norm1/bias',\n",
       " 'layer2.0.shortcut.0.weight': 'blockgroups_2/blocks_1/conv2/kernel',\n",
       " 'layer2.0.shortcut.1.weight': 'blockgroups_2/blocks_1/norm2/scale',\n",
       " 'layer2.0.shortcut.1.bias': 'blockgroups_2/blocks_1/norm2/bias',\n",
       " 'layer2.0.shortcut.1.running_mean': 'blockgroups_2/blocks_2/conv1/kernel',\n",
       " 'layer2.0.shortcut.1.running_var': 'blockgroups_2/blocks_2/norm1/scale',\n",
       " 'layer2.0.shortcut.1.num_batches_tracked': 'blockgroups_2/blocks_2/norm1/bias',\n",
       " 'layer2.1.conv1.weight': 'blockgroups_2/blocks_2/conv2/kernel',\n",
       " 'layer2.1.bn1.weight': 'blockgroups_2/blocks_2/norm2/scale',\n",
       " 'layer2.1.bn1.bias': 'blockgroups_2/blocks_2/norm2/bias',\n",
       " 'layer2.1.bn1.running_mean': 'dense/kernel',\n",
       " 'layer2.1.bn1.running_var': 'dense/bias'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(sd.keys(), spec.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as torch_F\n",
    "import torch.nn.init as torch_init\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, torch_nn.Linear) or isinstance(m, torch_nn.Conv2d):\n",
    "        torch_init.kaiming_normal_(m.weight)\n",
    "\n",
    "class torch_LambdaLayer(torch_nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(torch_LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class torch_BasicBlock(torch_nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(torch_BasicBlock, self).__init__()\n",
    "        self.conv1 = torch_nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = torch_nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch_nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = torch_nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch_nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "#             self.shortcut = LambdaLayer(lambda x:\n",
    "#                                         F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            self.shortcut = torch_nn.Sequential(\n",
    "                torch_nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                torch_nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch_F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch_F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class torch_ResNet(torch_nn.Module):\n",
    "    def __init__(self, block, num_blocks, w=1, num_classes=10):\n",
    "        super(torch_ResNet, self).__init__()\n",
    "        self.in_planes = w*16\n",
    "\n",
    "        self.conv1 = torch_nn.Conv2d(3, w*16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = torch_nn.BatchNorm2d(w*16)\n",
    "        self.layer1 = self._make_layer(block, w*16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, w*32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, w*64, num_blocks[2], stride=2)\n",
    "        self.linear = torch_nn.Linear(w*64, 512)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return torch_nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch_F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = torch_F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def torch_resnet20(w=1):\n",
    "    return torch_ResNet(torch_BasicBlock, [3, 3, 3], w=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import reduce as jax_reduce\n",
    "from flax import linen as jax_nn\n",
    "\n",
    "def reverse_compose(x, fs):\n",
    "  for f in fs:\n",
    "    x = f(x)\n",
    "  return x\n",
    "\n",
    "class Block(jax_nn.Module):\n",
    "  num_channels: int = None\n",
    "  strides: int = None\n",
    "\n",
    "  def setup(self):\n",
    "    self.conv1 = jax_nn.Conv(features=self.num_channels,\n",
    "                         kernel_size=(3, 3),\n",
    "                         strides=self.strides,\n",
    "                         use_bias=False)\n",
    "    self.norm1 = jax_nn.BatchNorm(momentum=0.9, use_running_average=True)\n",
    "    self.conv2 = jax_nn.Conv(features=self.num_channels, kernel_size=(3, 3), strides=1, use_bias=False)\n",
    "    self.norm2 = jax_nn.BatchNorm(momentum=0.9, use_running_average=True)\n",
    "\n",
    "    # When strides != 1, then it's 2, which means that we halve the width and height of the input, while doubling the\n",
    "    # number of channels. Therefore we need to correspondingly halve the width and height of the residuals/shortcut.\n",
    "    if self.strides != 1:\n",
    "      assert self.strides == 2\n",
    "\n",
    "      # Supposedly this is the original description, but it is not easily comaptible with our weight matching stuff\n",
    "      # since it plays games with the channel structure by padding things around.\n",
    "      # self.shortcut = lambda x: jnp.pad(x[:, ::2, ::2, :], (\n",
    "      #     (0, 0), (0, 0), (0, 0), (self.num_channels // 4, self.num_channels // 4)),\n",
    "      #                                   \"constant\",\n",
    "      #                                   constant_values=0)\n",
    "\n",
    "      # This is not the original, but is fairly common based on other implementations.\n",
    "      self.shortcut = jax_nn.Sequential([\n",
    "          jax_nn.Conv(features=self.num_channels,\n",
    "                  kernel_size=(3, 3),\n",
    "                  strides=self.strides,\n",
    "                  use_bias=False),\n",
    "          jax_nn.BatchNorm(momentum=0.9, use_running_average=True)\n",
    "      ])\n",
    "    else:\n",
    "      self.shortcut = lambda x: x\n",
    "\n",
    "  def __call__(self, x):\n",
    "    y = x\n",
    "    y = self.conv1(y)\n",
    "    y = self.norm1(y)\n",
    "    y = jax_nn.relu(y)\n",
    "    y = self.conv2(y)\n",
    "    y = self.norm2(y)\n",
    "    return jax_nn.relu(y + self.shortcut(x))\n",
    "\n",
    "class BlockGroup(jax_nn.Module):\n",
    "  num_channels: int = None\n",
    "  num_blocks: int = None\n",
    "  strides: int = None\n",
    "\n",
    "  def setup(self):\n",
    "    assert self.num_blocks > 0\n",
    "    self.blocks = (\n",
    "        [Block(num_channels=self.num_channels, strides=self.strides)] +\n",
    "        [Block(num_channels=self.num_channels, strides=1) for _ in range(self.num_blocks - 1)])\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return reverse_compose(x, self.blocks)\n",
    "\n",
    "class ResNet(jax_nn.Module):\n",
    "  blocks_per_group: int = None\n",
    "  num_classes: int = None\n",
    "  width_multiplier: int = 1\n",
    "\n",
    "  def setup(self):\n",
    "    wm = self.width_multiplier\n",
    "\n",
    "    self.conv1 = jax_nn.Conv(features=16 * wm, kernel_size=(3, 3), use_bias=False)\n",
    "    self.norm1 = jax_nn.BatchNorm(momentum=0.9, use_running_average=True)\n",
    "\n",
    "    channels_per_group = (16 * wm, 32 * wm, 64 * wm)\n",
    "    strides_per_group = (1, 2, 2)\n",
    "    self.blockgroups = [\n",
    "        BlockGroup(num_channels=c, num_blocks=b, strides=s)\n",
    "        for c, b, s in zip(channels_per_group, self.blocks_per_group, strides_per_group)\n",
    "    ]\n",
    "\n",
    "    self.dense = jax_nn.Dense(self.num_classes)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = jax_nn.relu(x)\n",
    "    x = reverse_compose(x, self.blockgroups)\n",
    "    x = jax_reduce(x, \"n h w c -> n c\", \"mean\")\n",
    "    x = self.dense(x)\n",
    "    # x = jax_nn.log_softmax(x)\n",
    "    return x\n",
    "\n",
    "def jax_resnet20(w=1):\n",
    "  return ResNet(BLOCKS_PER_GROUP[\"resnet20\"], num_classes=512, width_multiplier=w)\n",
    "BLOCKS_PER_GROUP = {\n",
    "    \"resnet20\": (3, 3, 3),\n",
    "    \"resnet32\": (5, 5, 5),\n",
    "    \"resnet44\": (7, 7, 7),\n",
    "    \"resnet56\": (9, 9, 9),\n",
    "    \"resnet110\": (18, 18, 18),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random as jax_random\n",
    "import jax\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_save_path = \"/srv/share/jbjorner3/checkpoints/REPAIR/resnet20x4_CIFAR50_clses[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49].pth.tar\"\n",
    "# model2_save_path = \"/srv/share/jbjorner3/checkpoints/REPAIR/resnet20x4_CIFAR50_clses[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99].pth.tar\"\n",
    "# sd = torch.load('model_1.th')[\"state_dict\"]\n",
    "# sd = {k: v.cpu() for k, v in sd.items()}\n",
    "# len(sd.keys())\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "torch_model = resnet20(w=4).to(DEVICE)\n",
    "sd = torch.load(model1_save_path, map_location=torch.device(DEVICE))\n",
    "sd = {k: v.cpu() for k, v in sd.items()}\n",
    "torch_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bn1.bias', [64]),\n",
       " ('bn1.running_mean', [64]),\n",
       " ('bn1.running_var', [64]),\n",
       " ('bn1.weight', [64]),\n",
       " ('conv1.weight', [64, 3, 3, 3]),\n",
       " ('layer1.0.bn1.bias', [64]),\n",
       " ('layer1.0.bn1.running_mean', [64]),\n",
       " ('layer1.0.bn1.running_var', [64]),\n",
       " ('layer1.0.bn1.weight', [64]),\n",
       " ('layer1.0.bn2.bias', [64]),\n",
       " ('layer1.0.bn2.running_mean', [64]),\n",
       " ('layer1.0.bn2.running_var', [64]),\n",
       " ('layer1.0.bn2.weight', [64]),\n",
       " ('layer1.0.conv1.weight', [64, 64, 3, 3]),\n",
       " ('layer1.0.conv2.weight', [64, 64, 3, 3]),\n",
       " ('layer1.1.bn1.bias', [64]),\n",
       " ('layer1.1.bn1.running_mean', [64]),\n",
       " ('layer1.1.bn1.running_var', [64]),\n",
       " ('layer1.1.bn1.weight', [64]),\n",
       " ('layer1.1.bn2.bias', [64]),\n",
       " ('layer1.1.bn2.running_mean', [64]),\n",
       " ('layer1.1.bn2.running_var', [64]),\n",
       " ('layer1.1.bn2.weight', [64]),\n",
       " ('layer1.1.conv1.weight', [64, 64, 3, 3]),\n",
       " ('layer1.1.conv2.weight', [64, 64, 3, 3]),\n",
       " ('layer1.2.bn1.bias', [64]),\n",
       " ('layer1.2.bn1.running_mean', [64]),\n",
       " ('layer1.2.bn1.running_var', [64]),\n",
       " ('layer1.2.bn1.weight', [64]),\n",
       " ('layer1.2.bn2.bias', [64]),\n",
       " ('layer1.2.bn2.running_mean', [64]),\n",
       " ('layer1.2.bn2.running_var', [64]),\n",
       " ('layer1.2.bn2.weight', [64]),\n",
       " ('layer1.2.conv1.weight', [64, 64, 3, 3]),\n",
       " ('layer1.2.conv2.weight', [64, 64, 3, 3]),\n",
       " ('layer2.0.bn1.bias', [128]),\n",
       " ('layer2.0.bn1.running_mean', [128]),\n",
       " ('layer2.0.bn1.running_var', [128]),\n",
       " ('layer2.0.bn1.weight', [128]),\n",
       " ('layer2.0.bn2.bias', [128]),\n",
       " ('layer2.0.bn2.running_mean', [128]),\n",
       " ('layer2.0.bn2.running_var', [128]),\n",
       " ('layer2.0.bn2.weight', [128]),\n",
       " ('layer2.0.conv1.weight', [128, 64, 3, 3]),\n",
       " ('layer2.0.conv2.weight', [128, 128, 3, 3]),\n",
       " ('layer2.0.shortcut.0.weight', [128, 64, 3, 3]),\n",
       " ('layer2.0.shortcut.1.bias', [128]),\n",
       " ('layer2.0.shortcut.1.running_mean', [128]),\n",
       " ('layer2.0.shortcut.1.running_var', [128]),\n",
       " ('layer2.0.shortcut.1.weight', [128]),\n",
       " ('layer2.1.bn1.bias', [128]),\n",
       " ('layer2.1.bn1.running_mean', [128]),\n",
       " ('layer2.1.bn1.running_var', [128]),\n",
       " ('layer2.1.bn1.weight', [128]),\n",
       " ('layer2.1.bn2.bias', [128]),\n",
       " ('layer2.1.bn2.running_mean', [128]),\n",
       " ('layer2.1.bn2.running_var', [128]),\n",
       " ('layer2.1.bn2.weight', [128]),\n",
       " ('layer2.1.conv1.weight', [128, 128, 3, 3]),\n",
       " ('layer2.1.conv2.weight', [128, 128, 3, 3]),\n",
       " ('layer2.2.bn1.bias', [128]),\n",
       " ('layer2.2.bn1.running_mean', [128]),\n",
       " ('layer2.2.bn1.running_var', [128]),\n",
       " ('layer2.2.bn1.weight', [128]),\n",
       " ('layer2.2.bn2.bias', [128]),\n",
       " ('layer2.2.bn2.running_mean', [128]),\n",
       " ('layer2.2.bn2.running_var', [128]),\n",
       " ('layer2.2.bn2.weight', [128]),\n",
       " ('layer2.2.conv1.weight', [128, 128, 3, 3]),\n",
       " ('layer2.2.conv2.weight', [128, 128, 3, 3]),\n",
       " ('layer3.0.bn1.bias', [256]),\n",
       " ('layer3.0.bn1.running_mean', [256]),\n",
       " ('layer3.0.bn1.running_var', [256]),\n",
       " ('layer3.0.bn1.weight', [256]),\n",
       " ('layer3.0.bn2.bias', [256]),\n",
       " ('layer3.0.bn2.running_mean', [256]),\n",
       " ('layer3.0.bn2.running_var', [256]),\n",
       " ('layer3.0.bn2.weight', [256]),\n",
       " ('layer3.0.conv1.weight', [256, 128, 3, 3]),\n",
       " ('layer3.0.conv2.weight', [256, 256, 3, 3]),\n",
       " ('layer3.0.shortcut.0.weight', [256, 128, 3, 3]),\n",
       " ('layer3.0.shortcut.1.bias', [256]),\n",
       " ('layer3.0.shortcut.1.running_mean', [256]),\n",
       " ('layer3.0.shortcut.1.running_var', [256]),\n",
       " ('layer3.0.shortcut.1.weight', [256]),\n",
       " ('layer3.1.bn1.bias', [256]),\n",
       " ('layer3.1.bn1.running_mean', [256]),\n",
       " ('layer3.1.bn1.running_var', [256]),\n",
       " ('layer3.1.bn1.weight', [256]),\n",
       " ('layer3.1.bn2.bias', [256]),\n",
       " ('layer3.1.bn2.running_mean', [256]),\n",
       " ('layer3.1.bn2.running_var', [256]),\n",
       " ('layer3.1.bn2.weight', [256]),\n",
       " ('layer3.1.conv1.weight', [256, 256, 3, 3]),\n",
       " ('layer3.1.conv2.weight', [256, 256, 3, 3]),\n",
       " ('layer3.2.bn1.bias', [256]),\n",
       " ('layer3.2.bn1.running_mean', [256]),\n",
       " ('layer3.2.bn1.running_var', [256]),\n",
       " ('layer3.2.bn1.weight', [256]),\n",
       " ('layer3.2.bn2.bias', [256]),\n",
       " ('layer3.2.bn2.running_mean', [256]),\n",
       " ('layer3.2.bn2.running_var', [256]),\n",
       " ('layer3.2.bn2.weight', [256]),\n",
       " ('layer3.2.conv1.weight', [256, 256, 3, 3]),\n",
       " ('layer3.2.conv2.weight', [256, 256, 3, 3]),\n",
       " ('linear.bias', [512]),\n",
       " ('linear.weight', [512, 256])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init torch_resnet20\n",
    "# print(torch_model)\n",
    "# print(dict(torch_model.state_dict()))\n",
    "torch_dict_sizes = jax.tree_util.tree_map(lambda x: list(x.shape), dict(torch_model.state_dict()))\n",
    "def not_bn_stat(x):\n",
    "    return \"num_batches_tracked\" not in x and 'mean' not in x and 'var' not in x\n",
    "# torch_list_sizes = list(item for item in torch_dict_sizes.items() if not not_bn_stat(item[0]) and \"num_batches\" not in item[0])\n",
    "torch_list_sizes = list(item for item in torch_dict_sizes.items() if \"num_batches\" not in item[0])\n",
    "print(torch_list_sizes.__len__())\n",
    "torch_list_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_stats': {'blockgroups_0': {'blocks_0': {'norm1': {'mean': (64,),\n",
      "                                                          'var': (64,)},\n",
      "                                                'norm2': {'mean': (64,),\n",
      "                                                          'var': (64,)}},\n",
      "                                   'blocks_1': {'norm1': {'mean': (64,),\n",
      "                                                          'var': (64,)},\n",
      "                                                'norm2': {'mean': (64,),\n",
      "                                                          'var': (64,)}},\n",
      "                                   'blocks_2': {'norm1': {'mean': (64,),\n",
      "                                                          'var': (64,)},\n",
      "                                                'norm2': {'mean': (64,),\n",
      "                                                          'var': (64,)}}},\n",
      "                 'blockgroups_1': {'blocks_0': {'norm1': {'mean': (128,),\n",
      "                                                          'var': (128,)},\n",
      "                                                'norm2': {'mean': (128,),\n",
      "                                                          'var': (128,)},\n",
      "                                                'shortcut': {'layers_1': {'mean': (128,),\n",
      "                                                                          'var': (128,)}}},\n",
      "                                   'blocks_1': {'norm1': {'mean': (128,),\n",
      "                                                          'var': (128,)},\n",
      "                                                'norm2': {'mean': (128,),\n",
      "                                                          'var': (128,)}},\n",
      "                                   'blocks_2': {'norm1': {'mean': (128,),\n",
      "                                                          'var': (128,)},\n",
      "                                                'norm2': {'mean': (128,),\n",
      "                                                          'var': (128,)}}},\n",
      "                 'blockgroups_2': {'blocks_0': {'norm1': {'mean': (256,),\n",
      "                                                          'var': (256,)},\n",
      "                                                'norm2': {'mean': (256,),\n",
      "                                                          'var': (256,)},\n",
      "                                                'shortcut': {'layers_1': {'mean': (256,),\n",
      "                                                                          'var': (256,)}}},\n",
      "                                   'blocks_1': {'norm1': {'mean': (256,),\n",
      "                                                          'var': (256,)},\n",
      "                                                'norm2': {'mean': (256,),\n",
      "                                                          'var': (256,)}},\n",
      "                                   'blocks_2': {'norm1': {'mean': (256,),\n",
      "                                                          'var': (256,)},\n",
      "                                                'norm2': {'mean': (256,),\n",
      "                                                          'var': (256,)}}},\n",
      "                 'norm1': {'mean': (64,), 'var': (64,)}},\n",
      " 'params': {'blockgroups_0': {'blocks_0': {'conv1': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'conv2': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'norm1': {'bias': (64,),\n",
      "                                                     'scale': (64,)},\n",
      "                                           'norm2': {'bias': (64,),\n",
      "                                                     'scale': (64,)}},\n",
      "                              'blocks_1': {'conv1': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'conv2': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'norm1': {'bias': (64,),\n",
      "                                                     'scale': (64,)},\n",
      "                                           'norm2': {'bias': (64,),\n",
      "                                                     'scale': (64,)}},\n",
      "                              'blocks_2': {'conv1': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'conv2': {'kernel': (3, 3, 64, 64)},\n",
      "                                           'norm1': {'bias': (64,),\n",
      "                                                     'scale': (64,)},\n",
      "                                           'norm2': {'bias': (64,),\n",
      "                                                     'scale': (64,)}}},\n",
      "            'blockgroups_1': {'blocks_0': {'conv1': {'kernel': (3, 3, 64, 128)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                128)},\n",
      "                                           'norm1': {'bias': (128,),\n",
      "                                                     'scale': (128,)},\n",
      "                                           'norm2': {'bias': (128,),\n",
      "                                                     'scale': (128,)},\n",
      "                                           'shortcut': {'layers_0': {'kernel': (3,\n",
      "                                                                                3,\n",
      "                                                                                64,\n",
      "                                                                                128)},\n",
      "                                                        'layers_1': {'bias': (128,),\n",
      "                                                                     'scale': (128,)}}},\n",
      "                              'blocks_1': {'conv1': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                128)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                128)},\n",
      "                                           'norm1': {'bias': (128,),\n",
      "                                                     'scale': (128,)},\n",
      "                                           'norm2': {'bias': (128,),\n",
      "                                                     'scale': (128,)}},\n",
      "                              'blocks_2': {'conv1': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                128)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                128)},\n",
      "                                           'norm1': {'bias': (128,),\n",
      "                                                     'scale': (128,)},\n",
      "                                           'norm2': {'bias': (128,),\n",
      "                                                     'scale': (128,)}}},\n",
      "            'blockgroups_2': {'blocks_0': {'conv1': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                128,\n",
      "                                                                256)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                256,\n",
      "                                                                256)},\n",
      "                                           'norm1': {'bias': (256,),\n",
      "                                                     'scale': (256,)},\n",
      "                                           'norm2': {'bias': (256,),\n",
      "                                                     'scale': (256,)},\n",
      "                                           'shortcut': {'layers_0': {'kernel': (3,\n",
      "                                                                                3,\n",
      "                                                                                128,\n",
      "                                                                                256)},\n",
      "                                                        'layers_1': {'bias': (256,),\n",
      "                                                                     'scale': (256,)}}},\n",
      "                              'blocks_1': {'conv1': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                256,\n",
      "                                                                256)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                256,\n",
      "                                                                256)},\n",
      "                                           'norm1': {'bias': (256,),\n",
      "                                                     'scale': (256,)},\n",
      "                                           'norm2': {'bias': (256,),\n",
      "                                                     'scale': (256,)}},\n",
      "                              'blocks_2': {'conv1': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                256,\n",
      "                                                                256)},\n",
      "                                           'conv2': {'kernel': (3,\n",
      "                                                                3,\n",
      "                                                                256,\n",
      "                                                                256)},\n",
      "                                           'norm1': {'bias': (256,),\n",
      "                                                     'scale': (256,)},\n",
      "                                           'norm2': {'bias': (256,),\n",
      "                                                     'scale': (256,)}}},\n",
      "            'conv1': {'kernel': (3, 3, 3, 64)},\n",
      "            'dense': {'bias': (512,), 'kernel': (256, 512)},\n",
      "            'norm1': {'bias': (64,), 'scale': (64,)}}}\n"
     ]
    }
   ],
   "source": [
    "torch_state_dict = dict(torch_model.state_dict())\n",
    "\n",
    "def fix_keys(old_key):\n",
    "    new_key = old_key\n",
    "    substitutions =[\n",
    "        (\"bn\", \"norm\"),\n",
    "        (\"layer\", \"blockgroups_\"),\n",
    "        (\"running_mean\", \"mean\"),\n",
    "        (\"running_var\", \"var\"),\n",
    "        (\"weight\", \"kernel\")\n",
    "    ]\n",
    "    for sub in substitutions:\n",
    "        new_key = new_key.replace(sub[0], sub[1])\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\", lambda x: f\"blockgroups_{int(x.group(1))-1}\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\\.(\\d)\\.\", \"blockgroups_\\g<1>.blocks_\\g<2>.\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.\", \"shortcut.layers_\", new_key)\n",
    "    new_key = re.sub(r\"norm(\\d).kernel\", \"norm\\g<1>.scale\", new_key)\n",
    "    return new_key\n",
    "def expand_dict(parent_dict, key, value):\n",
    "    # expand dict along periods of the key\n",
    "    keys = key.split(\".\")\n",
    "    if 'shortcut' in keys and 'layers_1' in keys and 'kernel' in keys:\n",
    "        keys[-1] = 'scale'\n",
    "    if 'linear' in keys:\n",
    "        keys[keys.index('linear')] = 'dense'\n",
    "    curr_dict = parent_dict\n",
    "    for new_key in keys[:-1]:\n",
    "        if curr_dict.get(new_key, None) == None:\n",
    "            curr_dict[new_key] = dict()\n",
    "        curr_dict = curr_dict[new_key]    \n",
    "    \n",
    "    curr_dict[keys[-1]] = value\n",
    "\n",
    "def fix_vals(old_key, old_val):\n",
    "    new_val = old_val.detach().cpu().numpy()\n",
    "    if \"conv\" in old_key or 'shortcut.0' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (2, 3, 1, 0))\n",
    "    elif 'linear.weight' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (1, 0))\n",
    "    return new_val\n",
    "        \n",
    "torch_state_dict = {fix_keys(k): fix_vals(k, v) for k, v in torch_state_dict.items()}\n",
    "\n",
    "torch_batch_stats = {k: v for k, v in torch_state_dict.items() if not not_bn_stat(k) and \"num_batches\" not in k}\n",
    "torch_params = {k: v for k, v in torch_state_dict.items() if not_bn_stat(k)}\n",
    "torch_batch_stats_expanded = dict()\n",
    "torch_params_expanded = dict()\n",
    "for k, v in torch_batch_stats.items():\n",
    "    expand_dict(torch_batch_stats_expanded, k, v)\n",
    "for k, v in torch_params.items():\n",
    "    expand_dict(torch_params_expanded, k, v)\n",
    "torch_params_dict = {\n",
    "        \"batch_stats\": torch_batch_stats_expanded, \n",
    "        \"params\": torch_params_expanded\n",
    "    }\n",
    "from pprint import pprint\n",
    "pprint(jax.tree_util.tree_map(lambda x: x.shape, torch_params_dict))\n",
    "# translate the shapes of the tensors to match with jax's formatting: https://flax.readthedocs.io/en/latest/advanced_topics/convert_pytorch_to_flax.html\n",
    "# model.apply(torch_params_dict, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the shapes of the tensors to match with jax's formatting: https://flax.readthedocs.io/en/latest/advanced_topics/convert_pytorch_to_flax.html\n",
    "# out = model.apply(torch_params_dict, x)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init jax_resnet20\n",
    "key1, key2 = jax_random.split(jax_random.PRNGKey(0), 2)\n",
    "x = jax_random.uniform(key1, (1, 32, 32, 3))\n",
    "\n",
    "jax_model = jax_resnet20(w=4)\n",
    "jax_params = jax_model.init(key2, x)\n",
    "y = jax_model.apply(jax_params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['params', 'batch_stats'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_params['params']['conv1']['kernel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    batch_stats: {\n",
       "        blockgroups_0: {\n",
       "            blocks_0: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([64], dtype=int32),\n",
       "                    var: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        blockgroups_1: {\n",
       "            blocks_0: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                shortcut: {\n",
       "                    layers_1: {\n",
       "                        mean: DeviceArray([128], dtype=int32),\n",
       "                        var: DeviceArray([128], dtype=int32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([128], dtype=int32),\n",
       "                    var: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        blockgroups_2: {\n",
       "            blocks_0: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                shortcut: {\n",
       "                    layers_1: {\n",
       "                        mean: DeviceArray([256], dtype=int32),\n",
       "                        var: DeviceArray([256], dtype=int32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                norm1: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    mean: DeviceArray([256], dtype=int32),\n",
       "                    var: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        norm1: {\n",
       "            mean: DeviceArray([64], dtype=int32),\n",
       "            var: DeviceArray([64], dtype=int32),\n",
       "        },\n",
       "    },\n",
       "    params: {\n",
       "        blockgroups_0: {\n",
       "            blocks_0: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([ 3,  3, 64, 64], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([64], dtype=int32),\n",
       "                    scale: DeviceArray([64], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        blockgroups_1: {\n",
       "            blocks_0: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3,  64, 128], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 128], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                shortcut: {\n",
       "                    layers_0: {\n",
       "                        kernel: DeviceArray([  3,   3,  64, 128], dtype=int32),\n",
       "                    },\n",
       "                    layers_1: {\n",
       "                        bias: DeviceArray([128], dtype=int32),\n",
       "                        scale: DeviceArray([128], dtype=int32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 128], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 128], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 128], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 128], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([128], dtype=int32),\n",
       "                    scale: DeviceArray([128], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        blockgroups_2: {\n",
       "            blocks_0: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3, 128, 256], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 256, 256], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                shortcut: {\n",
       "                    layers_0: {\n",
       "                        kernel: DeviceArray([  3,   3, 128, 256], dtype=int32),\n",
       "                    },\n",
       "                    layers_1: {\n",
       "                        bias: DeviceArray([256], dtype=int32),\n",
       "                        scale: DeviceArray([256], dtype=int32),\n",
       "                    },\n",
       "                },\n",
       "            },\n",
       "            blocks_1: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3, 256, 256], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 256, 256], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "            blocks_2: {\n",
       "                conv1: {\n",
       "                    kernel: DeviceArray([  3,   3, 256, 256], dtype=int32),\n",
       "                },\n",
       "                conv2: {\n",
       "                    kernel: DeviceArray([  3,   3, 256, 256], dtype=int32),\n",
       "                },\n",
       "                norm1: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "                norm2: {\n",
       "                    bias: DeviceArray([256], dtype=int32),\n",
       "                    scale: DeviceArray([256], dtype=int32),\n",
       "                },\n",
       "            },\n",
       "        },\n",
       "        conv1: {\n",
       "            kernel: DeviceArray([ 3,  3,  3, 64], dtype=int32),\n",
       "        },\n",
       "        dense: {\n",
       "            bias: DeviceArray([512], dtype=int32),\n",
       "            kernel: DeviceArray([256, 512], dtype=int32),\n",
       "        },\n",
       "        norm1: {\n",
       "            bias: DeviceArray([64], dtype=int32),\n",
       "            scale: DeviceArray([64], dtype=int32),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_params_dict = jax.tree_util.tree_map(lambda x: jax.numpy.array(x.shape),jax_params)\n",
    "count = 0\n",
    "def incrment_identity(x):\n",
    "    global count\n",
    "    count += 1\n",
    "    return x\n",
    "jax_params_dict = jax.tree_util.tree_map(incrment_identity, jax_params_dict)\n",
    "print(count)# __repr__()[:1000])\n",
    "jax_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "jax_params_permuted = pickle.load(open(\"/srv/share/gstoica3/checkpoints/REPAIR/flax_cifar50_2_permuted_to_1.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "# convert back to pytorch from jax.\n",
    "def collapse_dict(jax_params_dict):\n",
    "    torch_params_dict = dict()\n",
    "    \n",
    "    for old_dict in [jax_params_dict]:\n",
    "        # print(old_dict)\n",
    "        recursively_build_dict([], old_dict, torch_params_dict)\n",
    "    # pprint(torch_params_dict)\n",
    "    torch_params_dict = {fix_keys(k): v for k, v in torch_params_dict.items()}\n",
    "    torch_params_dict = {k: fix_vals(k, v) for k, v in torch_params_dict.items()}\n",
    "\n",
    "    return torch_params_dict\n",
    "def recursively_build_dict(old_keys, old_dict, new_dict):\n",
    "    if isinstance(old_dict, flax.core.frozen_dict.FrozenDict):\n",
    "        for old_key, old_val in old_dict.items():\n",
    "            recursively_build_dict(old_keys + [old_key], old_val, new_dict)\n",
    "    else:\n",
    "        # now we have an array to convert\n",
    "        new_dict[\".\".join(old_keys)] = old_dict\n",
    "def fix_keys(old_key):\n",
    "    new_key = old_key\n",
    "    new_key = re.sub(r\"norm(\\d).scale\", \"norm\\g<1>.kernel\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.layers_\", \"shortcut.\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.1\\.scale\", \"shortcut.1.weight\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\\.blocks_(\\d)\\.\", \"blockgroups_\\g<1>.\\g<2>.\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\", lambda x: f\"blockgroups_{int(x.group(1))+1}\", new_key)\n",
    "    substitutions =[(\"bn\", \"norm\"),(\"layer\", \"blockgroups_\"),(\"running_mean\", \"mean\"),(\"running_var\", \"var\"),(\"weight\", \"kernel\"),(\"linear\",\"dense\")]\n",
    "    for sub in substitutions[::-1]:\n",
    "        new_key = new_key.replace(sub[1], sub[0]) # in reverse order of old fix_keys\n",
    "    return new_key\n",
    "def fix_vals(old_key, old_val):\n",
    "    new_val = old_val\n",
    "    if \"conv\" in old_key or 'shortcut.0' in old_key:\n",
    "        # new_val = jnp.transpose(new_val, (2, 3, 1, 0))\n",
    "        new_val = jnp.transpose(new_val, (3, 2, 0, 1))\n",
    "    elif 'linear.weight' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (1, 0))\n",
    "    new_val = torch.tensor(np.array(new_val))\n",
    "    \n",
    "    return new_val\n",
    "\n",
    "# def find_permutation(bn_bias_permuted, bn_bias_not_permuted):\n",
    "#     # find the picking order necessary to index the not permuted to achieve the permuted.\n",
    "#     abs_dif_matrix = np.abs(bn_bias_permuted[None, :] - bn_bias_not_permuted[:, None])\n",
    "#     return np.argmin(abs_dif_matrix, axis=0)\n",
    "# def fix_batch_stats_permutations(new_torch_params_dict, old_torch_params_dict):\n",
    "#     # correct the batch stats based on the permutations in the weight and bias (should be the same?)\n",
    "#     def fix_batch_stats_val(k, v):\n",
    "#         if \"bn\" in k or \"shortcut.1\" in k:\n",
    "#             print(k)\n",
    "#             if \"running_mean\" in k or \"running_var\" in k:\n",
    "#                 bn_mean_key = k.replace(\"running_mean\", \"bias\").replace(\"running_var\", \"bias\")\n",
    "#                 perm_bn_mean = new_torch_params_dict[bn_mean_key]\n",
    "#                 not_perm_bn_mean = old_torch_params_dict[bn_mean_key]\n",
    "#                 perm_indexer = find_permutation(perm_bn_mean, not_perm_bn_mean)\n",
    "#                 return\n",
    "#             # remove the word bias or weight, and replace with running mean and running var to get access to these\n",
    "#         return v\n",
    "#     fixed_torch_params_dict = {k: v for k, v in new_torch_params_dict.items()}\n",
    "\n",
    "\n",
    "output_dict = collapse_dict(jax_params_permuted)\n",
    "print(len(output_dict))\n",
    "# to test set equality both ways.\n",
    "# pprint({item[0] for item in torch_list_sizes}.difference(output_dict.keys()))\n",
    "# pprint(set(output_dict.keys()).difference({item[0] for item in torch_list_sizes}))\n",
    "# pprint(jax.tree_util.tree_map(lambda x: x.shape, output_dict))\n",
    "# pprint()\n",
    "# correct_sizes_dict = {k:torch.Size(v) for k, v in torch_list_sizes}\n",
    "# for key, val in output_dict.items():\n",
    "#     if correct_sizes_dict[key] != val.shape:\n",
    "#         print(\"incorrect matching\", key, val.shape, correct_sizes_dict[key])\n",
    "#         print(correct_sizes_dict[key])\n",
    "#         print(val.shape)\n",
    "#         print(fix_vals(key, val.detach().numpy()).shape)\n",
    "# find and apply batch stats permutation based on the permutation of the bn bias.\n",
    "# jax_params.keys()\n",
    "output_dict\n",
    "# torch_list_sizes\n",
    "# old_torch_params_dict = dict(torch_model.state_dict())\n",
    "pickle.dump(output_dict, open(\"/srv/share/jbjorner3/checkpoints/REPAIR/pytorch_cifar50_2_permuted_to_1_jakob.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_resnet20(w=4).load_state_dict(output_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "69c61fc9b7d2d486ded6c3473d8882ad857e37220b85b60c4961b699e4b0f2eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
