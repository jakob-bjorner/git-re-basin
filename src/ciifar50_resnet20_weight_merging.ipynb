{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87632f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48f102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d22994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:20:44.419742: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-16 17:20:45.197959: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 17:20:45.198057: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 17:20:45.198065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from flax.serialization import from_bytes\n",
    "from jax import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from cifar100_resnet20_train import make_stuff\n",
    "from datasets import load_cifar100\n",
    "from resnet20 import BLOCKS_PER_GROUP, ResNet\n",
    "from utils import (ec2_get_instance_type, flatten_params, lerp, timeblock, unflatten_params)\n",
    "from weight_matching import (apply_permutation, resnet20_permutation_spec, weight_matching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc33f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 16 17:20:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A40                 On   | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   39C    P0    58W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e171806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879bfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"],\n",
    "                   num_classes=512,\n",
    "                   width_multiplier=4)\n",
    "model_b = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"],\n",
    "                   num_classes=512,\n",
    "                   width_multiplier=4)\n",
    "model_new = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"],\n",
    "                   num_classes=512,\n",
    "                   width_multiplier=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326ced38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 16 17:20:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A40                 On   | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   39C    P0    58W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b12ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    return pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b03c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sd = load_pickle(os.path.join(\n",
    "    '/srv/share/gstoica3/checkpoints/REPAIR',\n",
    "    'flax_cifar50_1.pkl'\n",
    "))\n",
    "\n",
    "b_sd = load_pickle(os.path.join(\n",
    "    '/srv/share/gstoica3/checkpoints/REPAIR',\n",
    "    'flax_cifar50_2.pkl'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28628e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import NamedTuple\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "from utils import rngmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220350a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 16 17:20:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A40                 On   | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   39C    P0    58W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a571cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationSpec(NamedTuple):\n",
    "    perm_to_axes: dict\n",
    "    axes_to_perm: dict\n",
    "\n",
    "# def mlp_permutation_spec(num_hidden_layers: int) -> PermutationSpec:\n",
    "#   \"\"\"We assume that one permutation cannot appear in two axes of the same weight array.\"\"\"\n",
    "#   assert num_hidden_layers >= 1\n",
    "#   return PermutationSpec(\n",
    "#       perm_to_axes={\n",
    "#           f\"P_{i}\": [(f\"Dense_{i}/kernel\", 1), (f\"Dense_{i}/bias\", 0), (f\"Dense_{i+1}/kernel\", 0)]\n",
    "#           for i in range(num_hidden_layers)\n",
    "#       },\n",
    "#       axes_to_perm={\n",
    "#           \"Dense_0/kernel\": (None, \"P_0\"),\n",
    "#           **{f\"Dense_{i}/kernel\": (f\"P_{i-1}\", f\"P_{i}\")\n",
    "#              for i in range(1, num_hidden_layers)},\n",
    "#           **{f\"Dense_{i}/bias\": (f\"P_{i}\", )\n",
    "#              for i in range(num_hidden_layers)},\n",
    "#           f\"Dense_{num_hidden_layers}/kernel\": (f\"P_{num_hidden_layers-1}\", None),\n",
    "#           f\"Dense_{num_hidden_layers}/bias\": (None, ),\n",
    "#       })\n",
    "\n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "    perm_to_axes = defaultdict(list)\n",
    "    for wk, axis_perms in axes_to_perm.items():\n",
    "        for axis, perm in enumerate(axis_perms):\n",
    "            if perm is not None:\n",
    "                perm_to_axes[perm].append((wk, axis))\n",
    "    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def mlp_permutation_spec(num_hidden_layers: int) -> PermutationSpec:\n",
    "    \"\"\"We assume that one permutation cannot appear in two axes of the same weight array.\"\"\"\n",
    "    assert num_hidden_layers >= 1\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "      \"Dense_0/kernel\": (None, \"P_0\"),\n",
    "      **{f\"Dense_{i}/kernel\": (f\"P_{i-1}\", f\"P_{i}\")\n",
    "         for i in range(1, num_hidden_layers)},\n",
    "      **{f\"Dense_{i}/bias\": (f\"P_{i}\", )\n",
    "         for i in range(num_hidden_layers)},\n",
    "      f\"Dense_{num_hidden_layers}/kernel\": (f\"P_{num_hidden_layers-1}\", None),\n",
    "      f\"Dense_{num_hidden_layers}/bias\": (None, ),\n",
    "    })\n",
    "\n",
    "def resnet20_permutation_spec() -> PermutationSpec:\n",
    "    conv = lambda name, p_in, p_out: {f\"{name}/kernel\": (None, None, p_in, p_out)}\n",
    "    norm = lambda name, p: {\n",
    "        f\"{name}/scale\": (p, ), \n",
    "        f\"{name}/bias\": (p, ), \n",
    "        f\"{name}/mean\": (p, ), \n",
    "        f\"{name}/var\": (p, )\n",
    "    }\n",
    "    dense = lambda name, p_in, p_out: {f\"{name}/kernel\": (p_in, p_out), f\"{name}/bias\": (p_out, )}\n",
    "\n",
    "    # This is for easy blocks that use a residual connection, without any change in the number of channels.\n",
    "    easyblock = lambda name, p: {\n",
    "      **conv(f\"{name}/conv1\", p, f\"P_{name}_inner\"),\n",
    "      **norm(f\"{name}/norm1\", f\"P_{name}_inner\"),\n",
    "      **conv(f\"{name}/conv2\", f\"P_{name}_inner\", p),\n",
    "      **norm(f\"{name}/norm2\", p)\n",
    "    }\n",
    "\n",
    "    # This is for blocks that use a residual connection, but change the number of channels via a Conv.\n",
    "    shortcutblock = lambda name, p_in, p_out: {\n",
    "      **conv(f\"{name}/conv1\", p_in, f\"P_{name}_inner\"),\n",
    "      **norm(f\"{name}/norm1\", f\"P_{name}_inner\"),\n",
    "      **conv(f\"{name}/conv2\", f\"P_{name}_inner\", p_out),\n",
    "      **norm(f\"{name}/norm2\", p_out),\n",
    "      **conv(f\"{name}/shortcut/layers_0\", p_in, p_out),\n",
    "      **norm(f\"{name}/shortcut/layers_1\", p_out),\n",
    "    }\n",
    "\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "      **conv(\"conv1\", None, \"P_bg0\"),\n",
    "      **norm(\"norm1\", \"P_bg0\"),\n",
    "      #\n",
    "      **easyblock(\"blockgroups_0/blocks_0\", \"P_bg0\"),\n",
    "      **easyblock(\"blockgroups_0/blocks_1\", \"P_bg0\"),\n",
    "      **easyblock(\"blockgroups_0/blocks_2\", \"P_bg0\"),\n",
    "      #\n",
    "      **shortcutblock(\"blockgroups_1/blocks_0\", \"P_bg0\", \"P_bg1\"),\n",
    "      **easyblock(\"blockgroups_1/blocks_1\", \"P_bg1\"),\n",
    "      **easyblock(\"blockgroups_1/blocks_2\", \"P_bg1\"),\n",
    "      #\n",
    "      **shortcutblock(\"blockgroups_2/blocks_0\", \"P_bg1\", \"P_bg2\"),\n",
    "      **easyblock(\"blockgroups_2/blocks_1\", \"P_bg2\"),\n",
    "      **easyblock(\"blockgroups_2/blocks_2\", \"P_bg2\"),\n",
    "      #\n",
    "      **dense(\"dense\", \"P_bg2\", None),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd86c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "  \"\"\"Get parameter `k` from `params`, with the permutations applied.\"\"\"\n",
    "    w = params[k]\n",
    "    for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "    # Skip the axis we're trying to permute.\n",
    "        if axis == except_axis:\n",
    "            continue\n",
    "\n",
    "        # None indicates that there is no permutation relevant to that axis.\n",
    "        if p is not None:\n",
    "            w = jnp.take(w, perm[p], axis=axis)\n",
    "\n",
    "    return w\n",
    "\n",
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "  \"\"\"Apply a `perm` to `params`.\"\"\"\n",
    "  return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
    "\n",
    "def weight_matching(rng,\n",
    "                    ps: PermutationSpec,\n",
    "                    params_a,\n",
    "                    params_b,\n",
    "                    max_iter=100,\n",
    "                    init_perm=None,\n",
    "                    silent=False):\n",
    "  \"\"\"Find a permutation of `params_b` to make them match `params_a`.\"\"\"\n",
    "  perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}\n",
    "\n",
    "  perm = {p: jnp.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
    "  perm_names = list(perm.keys())\n",
    "\n",
    "  for iteration in range(max_iter):\n",
    "    progress = False\n",
    "    for p_ix in random.permutation(rngmix(rng, iteration), len(perm_names)):\n",
    "      p = perm_names[p_ix]\n",
    "      n = perm_sizes[p]\n",
    "      A = jnp.zeros((n, n))\n",
    "      for wk, axis in ps.perm_to_axes[p]:\n",
    "        w_a = params_a[wk]\n",
    "        w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "        w_a = jnp.moveaxis(w_a, axis, 0).reshape((n, -1))\n",
    "        w_b = jnp.moveaxis(w_b, axis, 0).reshape((n, -1))\n",
    "        A += w_a @ w_b.T\n",
    "\n",
    "      ri, ci = linear_sum_assignment(A, maximize=True)\n",
    "      assert (ri == jnp.arange(len(ri))).all()\n",
    "\n",
    "      oldL = jnp.vdot(A, jnp.eye(n)[perm[p]])\n",
    "      newL = jnp.vdot(A, jnp.eye(n)[ci, :])\n",
    "      if not silent: print(f\"{iteration}/{p}: {newL - oldL}\")\n",
    "      progress = progress or newL > oldL + 1e-12\n",
    "\n",
    "      perm[p] = jnp.array(ci)\n",
    "\n",
    "    if not progress:\n",
    "      break\n",
    "\n",
    "  return perm\n",
    "\n",
    "def test_weight_matching():\n",
    "  \"\"\"If we just have a single hidden layer then it should converge after just one step.\"\"\"\n",
    "  ps = mlp_permutation_spec(num_hidden_layers=1)\n",
    "  rng = random.PRNGKey(123)\n",
    "  num_hidden = 10\n",
    "  shapes = {\n",
    "      \"Dense_0/kernel\": (2, num_hidden),\n",
    "      \"Dense_0/bias\": (num_hidden, ),\n",
    "      \"Dense_1/kernel\": (num_hidden, 3),\n",
    "      \"Dense_1/bias\": (3, )\n",
    "  }\n",
    "  params_a = {k: random.normal(rngmix(rng, f\"a-{k}\"), shape) for k, shape in shapes.items()}\n",
    "  params_b = {k: random.normal(rngmix(rng, f\"b-{k}\"), shape) for k, shape in shapes.items()}\n",
    "  perm = weight_matching(rng, ps, params_a, params_b)\n",
    "  print(perm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b62b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 16 17:20:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A40                 On   | 00000000:81:00.0 Off |                    0 |\n",
      "|  0%   37C    P8    22W / 300W |      0MiB / 45634MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb64e0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['params/conv1/kernel',\n",
       " 'params/norm1/scale',\n",
       " 'params/norm1/bias',\n",
       " 'params/blockgroups_0/blocks_0/conv1/kernel',\n",
       " 'params/blockgroups_0/blocks_0/norm1/scale',\n",
       " 'params/blockgroups_0/blocks_0/norm1/bias',\n",
       " 'params/blockgroups_0/blocks_0/conv2/kernel',\n",
       " 'params/blockgroups_0/blocks_0/norm2/scale',\n",
       " 'params/blockgroups_0/blocks_0/norm2/bias',\n",
       " 'params/blockgroups_0/blocks_1/conv1/kernel',\n",
       " 'params/blockgroups_0/blocks_1/norm1/scale',\n",
       " 'params/blockgroups_0/blocks_1/norm1/bias',\n",
       " 'params/blockgroups_0/blocks_1/conv2/kernel',\n",
       " 'params/blockgroups_0/blocks_1/norm2/scale',\n",
       " 'params/blockgroups_0/blocks_1/norm2/bias',\n",
       " 'params/blockgroups_0/blocks_2/conv1/kernel',\n",
       " 'params/blockgroups_0/blocks_2/norm1/scale',\n",
       " 'params/blockgroups_0/blocks_2/norm1/bias',\n",
       " 'params/blockgroups_0/blocks_2/conv2/kernel',\n",
       " 'params/blockgroups_0/blocks_2/norm2/scale',\n",
       " 'params/blockgroups_0/blocks_2/norm2/bias',\n",
       " 'params/blockgroups_1/blocks_0/conv1/kernel',\n",
       " 'params/blockgroups_1/blocks_0/norm1/scale',\n",
       " 'params/blockgroups_1/blocks_0/norm1/bias',\n",
       " 'params/blockgroups_1/blocks_0/conv2/kernel',\n",
       " 'params/blockgroups_1/blocks_0/norm2/scale',\n",
       " 'params/blockgroups_1/blocks_0/norm2/bias',\n",
       " 'params/blockgroups_1/blocks_0/shortcut/layers_0/kernel',\n",
       " 'params/blockgroups_1/blocks_0/shortcut/layers_1/scale',\n",
       " 'params/blockgroups_1/blocks_0/shortcut/layers_1/bias',\n",
       " 'params/blockgroups_1/blocks_1/conv1/kernel',\n",
       " 'params/blockgroups_1/blocks_1/norm1/scale',\n",
       " 'params/blockgroups_1/blocks_1/norm1/bias',\n",
       " 'params/blockgroups_1/blocks_1/conv2/kernel',\n",
       " 'params/blockgroups_1/blocks_1/norm2/scale',\n",
       " 'params/blockgroups_1/blocks_1/norm2/bias',\n",
       " 'params/blockgroups_1/blocks_2/conv1/kernel',\n",
       " 'params/blockgroups_1/blocks_2/norm1/scale',\n",
       " 'params/blockgroups_1/blocks_2/norm1/bias',\n",
       " 'params/blockgroups_1/blocks_2/conv2/kernel',\n",
       " 'params/blockgroups_1/blocks_2/norm2/scale',\n",
       " 'params/blockgroups_1/blocks_2/norm2/bias',\n",
       " 'params/blockgroups_2/blocks_0/conv1/kernel',\n",
       " 'params/blockgroups_2/blocks_0/norm1/scale',\n",
       " 'params/blockgroups_2/blocks_0/norm1/bias',\n",
       " 'params/blockgroups_2/blocks_0/conv2/kernel',\n",
       " 'params/blockgroups_2/blocks_0/norm2/scale',\n",
       " 'params/blockgroups_2/blocks_0/norm2/bias',\n",
       " 'params/blockgroups_2/blocks_0/shortcut/layers_0/kernel',\n",
       " 'params/blockgroups_2/blocks_0/shortcut/layers_1/scale',\n",
       " 'params/blockgroups_2/blocks_0/shortcut/layers_1/bias',\n",
       " 'params/blockgroups_2/blocks_1/conv1/kernel',\n",
       " 'params/blockgroups_2/blocks_1/norm1/scale',\n",
       " 'params/blockgroups_2/blocks_1/norm1/bias',\n",
       " 'params/blockgroups_2/blocks_1/conv2/kernel',\n",
       " 'params/blockgroups_2/blocks_1/norm2/scale',\n",
       " 'params/blockgroups_2/blocks_1/norm2/bias',\n",
       " 'params/blockgroups_2/blocks_2/conv1/kernel',\n",
       " 'params/blockgroups_2/blocks_2/norm1/scale',\n",
       " 'params/blockgroups_2/blocks_2/norm1/bias',\n",
       " 'params/blockgroups_2/blocks_2/conv2/kernel',\n",
       " 'params/blockgroups_2/blocks_2/norm2/scale',\n",
       " 'params/blockgroups_2/blocks_2/norm2/bias',\n",
       " 'params/dense/kernel',\n",
       " 'params/dense/bias']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in flatten_params(a_sd).keys() if i.startswith('params')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d8e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_spec = resnet20_permutation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4f00595",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 17:33:29.148591: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "2023-01-16 17:33:29.148662: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2023-01-16 17:33:29.149941: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream->parent()->GetBlasGemmAlgorithms(stream, &algorithms) \n",
      "*** Begin stack trace ***\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyMethodDef_RawFastCallKeywords\n",
      "\t_PyObject_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallDict\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\tPyObject_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyObject_FastCallDict\n",
      "\t_PyObject_FastCall_Prepend\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t\n",
      "\t_PyMethodDef_RawFastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyMethodDescr_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyObject_FastCallDict\n",
      "\t\n",
      "\tPyObject_Call\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t\n",
      "\t\n",
      "\t_PyObject_FastCallKeywords\n",
      "\t\n",
      "\t_PyMethodDef_RawFastCallDict\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t_PyFunction_FastCallKeywords\n",
      "\t\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "\t\n",
      "\t_PyMethodDef_RawFastCallKeywords\n",
      "\t_PyEval_EvalFrameDefault\n",
      "\t_PyEval_EvalCodeWithName\n",
      "*** End stack trace ***\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream->parent()->GetBlasGemmAlgorithms(stream, &algorithms) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36132/1052387241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# flatten_params(model_b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mflatten_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mflatten_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_36132/4268392891.py\u001b[0m in \u001b[0;36mweight_matching\u001b[0;34m(rng, ps, params_a, params_b, max_iter, init_perm, silent)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mw_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mw_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw_a\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_sum_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4934\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4935\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4936\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4937\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rejected_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4938\u001b[0m       raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[0;32m/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, built_c, options, host_callbacks)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m   \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;31m# TODO(phawkins): update users.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream->parent()->GetBlasGemmAlgorithms(stream, &algorithms) "
     ]
    }
   ],
   "source": [
    "final_permutation = weight_matching(\n",
    "    random.PRNGKey(0), permutation_spec,\n",
    "    # flatten_params(model_a), \n",
    "    # flatten_params(model_b)\n",
    "    flatten_params(a_sd['params']), \n",
    "    flatten_params(b_sd['params'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e3fd131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['norm1/mean', 'norm1/var', 'blockgroups_0/blocks_0/norm1/mean', 'blockgroups_0/blocks_0/norm1/var', 'blockgroups_0/blocks_0/norm2/mean', 'blockgroups_0/blocks_0/norm2/var', 'blockgroups_0/blocks_1/norm1/mean', 'blockgroups_0/blocks_1/norm1/var', 'blockgroups_0/blocks_1/norm2/mean', 'blockgroups_0/blocks_1/norm2/var', 'blockgroups_0/blocks_2/norm1/mean', 'blockgroups_0/blocks_2/norm1/var', 'blockgroups_0/blocks_2/norm2/mean', 'blockgroups_0/blocks_2/norm2/var', 'blockgroups_1/blocks_0/norm1/mean', 'blockgroups_1/blocks_0/norm1/var', 'blockgroups_1/blocks_0/norm2/mean', 'blockgroups_1/blocks_0/norm2/var', 'blockgroups_1/blocks_0/shortcut/layers_1/mean', 'blockgroups_1/blocks_0/shortcut/layers_1/var', 'blockgroups_1/blocks_1/norm1/mean', 'blockgroups_1/blocks_1/norm1/var', 'blockgroups_1/blocks_1/norm2/mean', 'blockgroups_1/blocks_1/norm2/var', 'blockgroups_1/blocks_2/norm1/mean', 'blockgroups_1/blocks_2/norm1/var', 'blockgroups_1/blocks_2/norm2/mean', 'blockgroups_1/blocks_2/norm2/var', 'blockgroups_2/blocks_0/norm1/mean', 'blockgroups_2/blocks_0/norm1/var', 'blockgroups_2/blocks_0/norm2/mean', 'blockgroups_2/blocks_0/norm2/var', 'blockgroups_2/blocks_0/shortcut/layers_1/mean', 'blockgroups_2/blocks_0/shortcut/layers_1/var', 'blockgroups_2/blocks_1/norm1/mean', 'blockgroups_2/blocks_1/norm1/var', 'blockgroups_2/blocks_1/norm2/mean', 'blockgroups_2/blocks_1/norm2/var', 'blockgroups_2/blocks_2/norm1/mean', 'blockgroups_2/blocks_2/norm1/var', 'blockgroups_2/blocks_2/norm2/mean', 'blockgroups_2/blocks_2/norm2/var'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_params(a_sd['batch_stats']).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6a02ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1/kernel', 'norm1/scale', 'norm1/bias', 'blockgroups_0/blocks_0/conv1/kernel', 'blockgroups_0/blocks_0/norm1/scale', 'blockgroups_0/blocks_0/norm1/bias', 'blockgroups_0/blocks_0/conv2/kernel', 'blockgroups_0/blocks_0/norm2/scale', 'blockgroups_0/blocks_0/norm2/bias', 'blockgroups_0/blocks_1/conv1/kernel', 'blockgroups_0/blocks_1/norm1/scale', 'blockgroups_0/blocks_1/norm1/bias', 'blockgroups_0/blocks_1/conv2/kernel', 'blockgroups_0/blocks_1/norm2/scale', 'blockgroups_0/blocks_1/norm2/bias', 'blockgroups_0/blocks_2/conv1/kernel', 'blockgroups_0/blocks_2/norm1/scale', 'blockgroups_0/blocks_2/norm1/bias', 'blockgroups_0/blocks_2/conv2/kernel', 'blockgroups_0/blocks_2/norm2/scale', 'blockgroups_0/blocks_2/norm2/bias', 'blockgroups_1/blocks_0/conv1/kernel', 'blockgroups_1/blocks_0/norm1/scale', 'blockgroups_1/blocks_0/norm1/bias', 'blockgroups_1/blocks_0/conv2/kernel', 'blockgroups_1/blocks_0/norm2/scale', 'blockgroups_1/blocks_0/norm2/bias', 'blockgroups_1/blocks_0/shortcut/layers_0/kernel', 'blockgroups_1/blocks_0/shortcut/layers_1/scale', 'blockgroups_1/blocks_0/shortcut/layers_1/bias', 'blockgroups_1/blocks_1/conv1/kernel', 'blockgroups_1/blocks_1/norm1/scale', 'blockgroups_1/blocks_1/norm1/bias', 'blockgroups_1/blocks_1/conv2/kernel', 'blockgroups_1/blocks_1/norm2/scale', 'blockgroups_1/blocks_1/norm2/bias', 'blockgroups_1/blocks_2/conv1/kernel', 'blockgroups_1/blocks_2/norm1/scale', 'blockgroups_1/blocks_2/norm1/bias', 'blockgroups_1/blocks_2/conv2/kernel', 'blockgroups_1/blocks_2/norm2/scale', 'blockgroups_1/blocks_2/norm2/bias', 'blockgroups_2/blocks_0/conv1/kernel', 'blockgroups_2/blocks_0/norm1/scale', 'blockgroups_2/blocks_0/norm1/bias', 'blockgroups_2/blocks_0/conv2/kernel', 'blockgroups_2/blocks_0/norm2/scale', 'blockgroups_2/blocks_0/norm2/bias', 'blockgroups_2/blocks_0/shortcut/layers_0/kernel', 'blockgroups_2/blocks_0/shortcut/layers_1/scale', 'blockgroups_2/blocks_0/shortcut/layers_1/bias', 'blockgroups_2/blocks_1/conv1/kernel', 'blockgroups_2/blocks_1/norm1/scale', 'blockgroups_2/blocks_1/norm1/bias', 'blockgroups_2/blocks_1/conv2/kernel', 'blockgroups_2/blocks_1/norm2/scale', 'blockgroups_2/blocks_1/norm2/bias', 'blockgroups_2/blocks_2/conv1/kernel', 'blockgroups_2/blocks_2/norm1/scale', 'blockgroups_2/blocks_2/norm1/bias', 'blockgroups_2/blocks_2/conv2/kernel', 'blockgroups_2/blocks_2/norm2/scale', 'blockgroups_2/blocks_2/norm2/bias', 'dense/kernel', 'dense/bias'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_params(a_sd['params']).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "700e2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sd_union = a_sd['params']\n",
    "a_sd_union.update(a_sd['batch_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8001b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
