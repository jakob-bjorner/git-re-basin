{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Pytorch to Flax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share/gstoica3/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.serialization import from_bytes\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from resnet20_jax import BLOCKS_PER_GROUP, ResNet\n",
    "from flax.traverse_util import flatten_dict\n",
    "import numpy as np\n",
    "from utils import flatten_params, unflatten_params\n",
    "from weight_matching import resnet20_permutation_spec\n",
    "from resnet20_torch import resnet20\n",
    "import os\n",
    "from jax import random as jax_random\n",
    "import re\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from flax.serialization import from_bytes\n",
    "from jax import random\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from resnet20 import BLOCKS_PER_GROUP, ResNet\n",
    "from utils import (ec2_get_instance_type, flatten_params, lerp, timeblock, unflatten_params)\n",
    "from weight_matching import (apply_permutation, resnet20_permutation_spec, weight_matching)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import flax\n",
    "from collections import defaultdict\n",
    "from typing import NamedTuple\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import pdb\n",
    "\n",
    "from utils import rngmix\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(str_splits):\n",
    "    pairs = []\n",
    "    for i, str_split_i in enumerate(str_splits):\n",
    "        if '_' not in str_split_i: continue\n",
    "        split_i = set([int(k) for k in str_split_i.split('_')])\n",
    "        for str_split_j in str_splits[i+1:]:\n",
    "            if '_' not in str_split_j: continue\n",
    "            split_j = set([int(k) for k in str_split_j.split('_')])\n",
    "            if len(split_i.intersection(split_j)) == 0:\n",
    "                pairs.append((str_split_i, str_split_j))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def split_str_to_ints(split):\n",
    "    return [int(i) for i in split.split('_')]\n",
    "\n",
    "\n",
    "def is_valid_pair(model_dir, pair, model_type):\n",
    "    paths = os.listdir(os.path.join(model_dir, pair[0]))\n",
    "    for path in paths:\n",
    "        if model_type in path:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def torch_to_linen(torch_params, get_flax_keys):\n",
    "    \"\"\"Convert PyTorch parameters to Linen nested dictionaries\"\"\"\n",
    "\n",
    "    def add_to_params(params_dict, nested_keys, param, is_conv=False):\n",
    "        if len(nested_keys) == 1:\n",
    "            key, = nested_keys\n",
    "            try:\n",
    "                params_dict[key] = np.transpose(param, (2, 3, 1, 0)) if is_conv else np.transpose(param)\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "        else:\n",
    "            assert len(nested_keys) > 1\n",
    "            first_key = nested_keys[0]\n",
    "            if first_key not in params_dict:\n",
    "                params_dict[first_key] = {}\n",
    "            add_to_params(params_dict[first_key], nested_keys[1:], param, ('conv' in first_key and \\\n",
    "                                                                         nested_keys[-1] != 'bias'))\n",
    "\n",
    "    flax_params = {'params': {}, 'batch_stats': {}}\n",
    "    for key, tensor in torch_params.items():\n",
    "        flax_keys = get_flax_keys(key.split('.'))\n",
    "        if flax_keys[-1] is not None:\n",
    "            if flax_keys[-1] in ('mean', 'var'):\n",
    "                add_to_params(flax_params['batch_stats'], flax_keys, tensor.detach().numpy())\n",
    "            else:\n",
    "                add_to_params(flax_params['params'], flax_keys, tensor.detach().numpy())\n",
    "\n",
    "    return flax_params\n",
    "\n",
    "\n",
    "def fix_keys(old_key):\n",
    "    new_key = old_key\n",
    "    substitutions =[\n",
    "        (\"bn\", \"norm\"),\n",
    "        (\"layer\", \"blockgroups_\"),\n",
    "        (\"running_mean\", \"mean\"),\n",
    "        (\"running_var\", \"var\"),\n",
    "        (\"weight\", \"kernel\")\n",
    "    ]\n",
    "    for sub in substitutions:\n",
    "        new_key = new_key.replace(sub[0], sub[1])\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\", lambda x: f\"blockgroups_{int(x.group(1))-1}\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\\.(\\d)\\.\", \"blockgroups_\\g<1>.blocks_\\g<2>.\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.\", \"shortcut.layers_\", new_key)\n",
    "    new_key = re.sub(r\"norm(\\d).kernel\", \"norm\\g<1>.scale\", new_key)\n",
    "    return new_key\n",
    "\n",
    "\n",
    "def expand_dict(parent_dict, key, value):\n",
    "    # expand dict along periods of the key\n",
    "    keys = key.split(\".\")\n",
    "    if 'shortcut' in keys and 'layers_1' in keys and 'kernel' in keys:\n",
    "        keys[-1] = 'scale'\n",
    "    if 'linear' in keys:\n",
    "        keys[keys.index('linear')] = 'dense'\n",
    "    curr_dict = parent_dict\n",
    "    for new_key in keys[:-1]:\n",
    "        if curr_dict.get(new_key, None) == None:\n",
    "            curr_dict[new_key] = dict()\n",
    "        curr_dict = curr_dict[new_key]    \n",
    "    curr_dict[keys[-1]] = value\n",
    "\n",
    "\n",
    "def fix_vals(old_key, old_val):\n",
    "    new_val = old_val.detach().cpu().numpy()\n",
    "    if \"conv\" in old_key or 'shortcut.0' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (2, 3, 1, 0))\n",
    "    elif 'linear.weight' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (1, 0))\n",
    "    return new_val\n",
    "\n",
    "\n",
    "def convert_torch_sd_to_flax_sd(torch_state_dict):\n",
    "    torch_state_dict = {fix_keys(k): fix_vals(k, v) for k, v in torch_state_dict.items()}\n",
    "\n",
    "    torch_batch_stats = {k: v for k, v in torch_state_dict.items() if not not_bn_stat(k) and \"num_batches\" not in k}\n",
    "    torch_params = {k: v for k, v in torch_state_dict.items() if not_bn_stat(k)}\n",
    "    torch_batch_stats_expanded = dict()\n",
    "    torch_params_expanded = dict()\n",
    "    for k, v in torch_batch_stats.items():\n",
    "        expand_dict(torch_batch_stats_expanded, k, v)\n",
    "    for k, v in torch_params.items():\n",
    "        expand_dict(torch_params_expanded, k, v)\n",
    "    flax_dict = {\n",
    "            \"batch_stats\": torch_batch_stats_expanded, \n",
    "            \"params\": torch_params_expanded\n",
    "        }\n",
    "    return flax_dict\n",
    "\n",
    "def not_bn_stat(x):\n",
    "    return \"num_batches_tracked\" not in x and 'mean' not in x and 'var' not in x\n",
    "\n",
    "class PermutationSpec(NamedTuple):\n",
    "    perm_to_axes: dict\n",
    "    axes_to_perm: dict\n",
    "    \n",
    "\n",
    "\n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "    perm_to_axes = defaultdict(list)\n",
    "    for wk, axis_perms in axes_to_perm.items():\n",
    "        for axis, perm in enumerate(axis_perms):\n",
    "            if perm is not None:\n",
    "                perm_to_axes[perm].append((wk, axis))\n",
    "    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def resnet20_permutation_spec() -> PermutationSpec:\n",
    "    conv = lambda name, p_in, p_out: {f\"{name}/kernel\": (None, None, p_in, p_out)}\n",
    "    norm = lambda name, p: {\n",
    "        f\"{name}/scale\": (p, ), \n",
    "        f\"{name}/bias\": (p, ), \n",
    "        f\"{name}/mean\": (p, ), \n",
    "        f\"{name}/var\": (p, )\n",
    "    }\n",
    "    dense = lambda name, p_in, p_out: {f\"{name}/kernel\": (p_in, p_out), f\"{name}/bias\": (p_out, )}\n",
    "\n",
    "    # This is for easy blocks that use a residual connection, without any change in the number of channels.\n",
    "    easyblock = lambda name, p: {\n",
    "      **conv(f\"{name}/conv1\", p, f\"P_{name}_inner\"),\n",
    "      **norm(f\"{name}/norm1\", f\"P_{name}_inner\"),\n",
    "      **conv(f\"{name}/conv2\", f\"P_{name}_inner\", p),\n",
    "      **norm(f\"{name}/norm2\", p)\n",
    "    }\n",
    "\n",
    "    # This is for blocks that use a residual connection, but change the number of channels via a Conv.\n",
    "    shortcutblock = lambda name, p_in, p_out: {\n",
    "      **conv(f\"{name}/conv1\", p_in, f\"P_{name}_inner\"),\n",
    "      **norm(f\"{name}/norm1\", f\"P_{name}_inner\"),\n",
    "      **conv(f\"{name}/conv2\", f\"P_{name}_inner\", p_out),\n",
    "      **norm(f\"{name}/norm2\", p_out),\n",
    "      **conv(f\"{name}/shortcut/layers_0\", p_in, p_out),\n",
    "      **norm(f\"{name}/shortcut/layers_1\", p_out),\n",
    "    }\n",
    "\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "      **conv(\"conv1\", None, \"P_bg0\"),\n",
    "      **norm(\"norm1\", \"P_bg0\"),\n",
    "      #\n",
    "      **easyblock(\"blockgroups_0/blocks_0\", \"P_bg0\"),\n",
    "      **easyblock(\"blockgroups_0/blocks_1\", \"P_bg0\"),\n",
    "      **easyblock(\"blockgroups_0/blocks_2\", \"P_bg0\"),\n",
    "      #\n",
    "      **shortcutblock(\"blockgroups_1/blocks_0\", \"P_bg0\", \"P_bg1\"),\n",
    "      **easyblock(\"blockgroups_1/blocks_1\", \"P_bg1\"),\n",
    "      **easyblock(\"blockgroups_1/blocks_2\", \"P_bg1\"),\n",
    "      #\n",
    "      **shortcutblock(\"blockgroups_2/blocks_0\", \"P_bg1\", \"P_bg2\"),\n",
    "      **easyblock(\"blockgroups_2/blocks_1\", \"P_bg2\"),\n",
    "      **easyblock(\"blockgroups_2/blocks_2\", \"P_bg2\"),\n",
    "      #\n",
    "      **dense(\"dense\", \"P_bg2\", None),\n",
    "    })\n",
    "    \n",
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "    \"\"\"Get parameter `k` from `params`, with the permutations applied.\"\"\"\n",
    "    w = params[k]\n",
    "    for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "    # Skip the axis we're trying to permute.\n",
    "        if axis == except_axis:\n",
    "            continue\n",
    "\n",
    "        # None indicates that there is no permutation relevant to that axis.\n",
    "        if p is not None:\n",
    "            w = jnp.take(w, perm[p], axis=axis)\n",
    "\n",
    "    return w\n",
    "\n",
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "  \"\"\"Apply a `perm` to `params`.\"\"\"\n",
    "  return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
    "\n",
    "def weight_matching(rng,\n",
    "                    ps: PermutationSpec,\n",
    "                    params_a,\n",
    "                    params_b,\n",
    "                    max_iter=100,\n",
    "                    init_perm=None,\n",
    "                    silent=False):\n",
    "  \"\"\"Find a permutation of `params_b` to make them match `params_a`.\"\"\"\n",
    "  perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}\n",
    "\n",
    "  perm = {p: jnp.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
    "  perm_names = list(perm.keys())\n",
    "  \n",
    "  for iteration in tqdm(range(max_iter)):\n",
    "    progress = False\n",
    "    for p_ix in random.permutation(rngmix(rng, iteration), len(perm_names)):\n",
    "      p = perm_names[p_ix]\n",
    "      n = perm_sizes[p]\n",
    "      A = jnp.zeros((n, n))\n",
    "      for wk, axis in ps.perm_to_axes[p]:\n",
    "        # pdb.set_trace()\n",
    "        try:\n",
    "          w_a = params_a[wk]\n",
    "        except:\n",
    "          pdb.set_trace()\n",
    "        w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "        w_a = jnp.moveaxis(w_a, axis, 0).reshape((n, -1))\n",
    "        w_b = jnp.moveaxis(w_b, axis, 0).reshape((n, -1))\n",
    "        A += w_a @ w_b.T\n",
    "\n",
    "      ri, ci = linear_sum_assignment(A, maximize=True)\n",
    "      assert (ri == jnp.arange(len(ri))).all()\n",
    "\n",
    "      oldL = jnp.vdot(A, jnp.eye(n)[perm[p]])\n",
    "      newL = jnp.vdot(A, jnp.eye(n)[ci, :])\n",
    "      if not silent: print(f\"{iteration}/{p}: {newL - oldL}\")\n",
    "      progress = progress or newL > oldL + 1e-12\n",
    "\n",
    "      perm[p] = jnp.array(ci)\n",
    "\n",
    "    if not progress:\n",
    "      break\n",
    "\n",
    "  return perm\n",
    "\n",
    "def load_pickle(path):\n",
    "    return pickle.load(open(path, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"],\n",
    "                   num_classes=512,\n",
    "                   width_multiplier=4)\n",
    "\n",
    "key1 , key2 = random.split(random.PRNGKey(0))\n",
    "model_params = model.init(key2, jnp.zeros((1, 32, 32, 3)))\n",
    "params = flatten_params(model_params)\n",
    "spec = resnet20_permutation_spec().axes_to_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pair = 4\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model_dir = '/srv/share2/gstoica3/checkpoints/cifar50_traincliphead/'\n",
    "model_name = 'resnet20x4'\n",
    "pair = [pair for pair in find_pairs(os.listdir(model_dir)) if is_valid_pair(model_dir, pair, model_name)][eval_pair]\n",
    "model_save_paths = [os.path.join(model_dir, split, f'{model_name}_v0.pth.tar') for split in pair]\n",
    "\n",
    "\n",
    "model1 = resnet20(w=4).to(DEVICE)\n",
    "sd = torch.load(model_save_paths[0], map_location=torch.device(DEVICE))\n",
    "sd = {k: v.cpu() for k, v in sd.items()}\n",
    "model1.load_state_dict(sd)\n",
    "\n",
    "model2 = resnet20(w=4).to(DEVICE)\n",
    "sd = torch.load(model_save_paths[1], map_location=torch.device(DEVICE))\n",
    "sd = {k: v.cpu() for k, v in sd.items()}\n",
    "model2.load_state_dict(sd)\n",
    "\n",
    "model1_state_dict = dict(model1.state_dict())\n",
    "model2_state_dict = dict(model2.state_dict())\n",
    "\n",
    "model1_flax_sd = convert_torch_sd_to_flax_sd(model1_state_dict)\n",
    "model2_flax_sd = convert_torch_sd_to_flax_sd(model2_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Git Rebasin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/P_blockgroups_1/blocks_1_inner: 7.041866779327393\n",
      "0/P_blockgroups_1/blocks_2_inner: 7.580317974090576\n",
      "0/P_blockgroups_1/blocks_0_inner: 5.360034465789795\n",
      "0/P_blockgroups_2/blocks_0_inner: 12.64707088470459\n",
      "0/P_blockgroups_0/blocks_0_inner: 4.85614013671875\n",
      "0/P_blockgroups_2/blocks_2_inner: 15.293581008911133\n",
      "0/P_bg0: 60.49738311767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:36,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/P_bg1: 14.953239440917969\n",
      "0/P_blockgroups_0/blocks_1_inner: 5.483088493347168\n",
      "0/P_blockgroups_2/blocks_1_inner: 15.382516860961914\n",
      "0/P_blockgroups_0/blocks_2_inner: 2.7440671920776367\n",
      "0/P_bg2: 4.778938293457031\n",
      "1/P_blockgroups_2/blocks_0_inner: 5.143930435180664\n",
      "1/P_blockgroups_0/blocks_0_inner: 5.997053623199463\n",
      "1/P_blockgroups_2/blocks_2_inner: 3.9344635009765625\n",
      "1/P_bg0: 3.6104812622070312\n",
      "1/P_bg1: 0.32691192626953125\n",
      "1/P_blockgroups_1/blocks_0_inner: 5.436430931091309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:35,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/P_bg2: 0.6397781372070312\n",
      "1/P_blockgroups_1/blocks_2_inner: 1.9989070892333984\n",
      "1/P_blockgroups_2/blocks_1_inner: 4.427860260009766\n",
      "1/P_blockgroups_0/blocks_2_inner: 1.3585796356201172\n",
      "1/P_blockgroups_0/blocks_1_inner: 1.1153879165649414\n",
      "1/P_blockgroups_1/blocks_1_inner: 4.134324073791504\n",
      "2/P_blockgroups_0/blocks_2_inner: 0.0\n",
      "2/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "2/P_blockgroups_1/blocks_2_inner: 0.0\n",
      "2/P_bg1: 0.79278564453125\n",
      "2/P_blockgroups_2/blocks_1_inner: 0.0\n",
      "2/P_bg2: 0.244598388671875\n",
      "2/P_blockgroups_0/blocks_0_inner: 0.08503055572509766\n",
      "2/P_bg0: 1.1651229858398438\n",
      "2/P_blockgroups_1/blocks_0_inner: 0.17174053192138672\n",
      "2/P_blockgroups_2/blocks_2_inner: 1.54412841796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:01<00:35,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/P_blockgroups_2/blocks_0_inner: 2.198131561279297\n",
      "2/P_blockgroups_0/blocks_1_inner: 0.08898067474365234\n",
      "3/P_bg0: 0.0\n",
      "3/P_blockgroups_2/blocks_0_inner: 0.0\n",
      "3/P_blockgroups_2/blocks_2_inner: 0.0\n",
      "3/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "3/P_blockgroups_1/blocks_2_inner: 0.3057107925415039\n",
      "3/P_bg1: 0.000457763671875\n",
      "3/P_blockgroups_0/blocks_0_inner: 0.006317138671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:01<00:34,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/P_blockgroups_1/blocks_0_inner: 0.0\n",
      "3/P_blockgroups_1/blocks_1_inner: 0.24361228942871094\n",
      "3/P_blockgroups_0/blocks_2_inner: 0.1865367889404297\n",
      "3/P_bg2: 0.08533477783203125\n",
      "3/P_blockgroups_2/blocks_1_inner: 0.9787864685058594\n",
      "4/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "4/P_blockgroups_1/blocks_2_inner: 0.00106048583984375\n",
      "4/P_bg2: 0.0570526123046875\n",
      "4/P_bg0: 0.01183319091796875\n",
      "4/P_blockgroups_1/blocks_0_inner: 0.01424407958984375\n",
      "4/P_blockgroups_0/blocks_2_inner: 0.0047893524169921875\n",
      "4/P_blockgroups_2/blocks_2_inner: 0.9189910888671875\n",
      "4/P_blockgroups_2/blocks_1_inner: 0.5577049255371094\n",
      "4/P_bg1: 0.04776763916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:01<00:34,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/P_blockgroups_1/blocks_1_inner: 0.07779121398925781\n",
      "4/P_blockgroups_2/blocks_0_inner: 0.4142265319824219\n",
      "4/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "5/P_bg0: 0.019683837890625\n",
      "5/P_blockgroups_2/blocks_1_inner: 0.0\n",
      "5/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "5/P_blockgroups_1/blocks_2_inner: 0.1697835922241211\n",
      "5/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "5/P_blockgroups_1/blocks_0_inner: 0.04564094543457031\n",
      "5/P_bg2: 0.0482330322265625\n",
      "5/P_blockgroups_2/blocks_0_inner: 0.2035999298095703\n",
      "5/P_blockgroups_0/blocks_2_inner: 0.0018587112426757812\n",
      "5/P_blockgroups_2/blocks_2_inner: 0.45946502685546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:02<00:35,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/P_bg1: 0.1015777587890625\n",
      "5/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "6/P_blockgroups_2/blocks_1_inner: 0.5788841247558594\n",
      "6/P_blockgroups_2/blocks_2_inner: 0.0\n",
      "6/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "6/P_blockgroups_0/blocks_2_inner: 0.0\n",
      "6/P_blockgroups_1/blocks_0_inner: 0.012342453002929688\n",
      "6/P_blockgroups_0/blocks_1_inner: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:02<00:34,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/P_bg1: 0.00377655029296875\n",
      "6/P_blockgroups_1/blocks_1_inner: 0.09410667419433594\n",
      "6/P_blockgroups_1/blocks_2_inner: 0.07547855377197266\n",
      "6/P_bg0: 0.0\n",
      "6/P_bg2: 0.018280029296875\n",
      "6/P_blockgroups_2/blocks_0_inner: 0.15760231018066406\n",
      "7/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "7/P_blockgroups_0/blocks_2_inner: 0.0\n",
      "7/P_bg2: 0.0\n",
      "7/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "7/P_blockgroups_1/blocks_2_inner: 0.0\n",
      "7/P_blockgroups_2/blocks_1_inner: 0.10795974731445312\n",
      "7/P_blockgroups_1/blocks_0_inner: 0.028123855590820312\n",
      "7/P_blockgroups_2/blocks_2_inner: 0.24777984619140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:02<00:34,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/P_blockgroups_2/blocks_0_inner: 0.0\n",
      "7/P_bg1: 6.866455078125e-05\n",
      "7/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "7/P_bg0: 0.0\n",
      "8/P_blockgroups_2/blocks_1_inner: 0.0\n",
      "8/P_bg2: 0.063385009765625\n",
      "8/P_bg0: 0.0\n",
      "8/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "8/P_blockgroups_0/blocks_2_inner: 0.0\n",
      "8/P_blockgroups_2/blocks_0_inner: 0.1425914764404297\n",
      "8/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "8/P_blockgroups_1/blocks_0_inner: 0.030544281005859375\n",
      "8/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "8/P_blockgroups_1/blocks_2_inner: 0.020730972290039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:03<00:32,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/P_bg1: 0.0\n",
      "8/P_blockgroups_2/blocks_2_inner: 0.25704193115234375\n",
      "9/P_blockgroups_2/blocks_1_inner: 0.215667724609375\n",
      "9/P_bg0: 0.0124664306640625\n",
      "9/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "9/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "9/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "9/P_blockgroups_2/blocks_2_inner: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:03<00:32,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/P_bg2: 0.030914306640625\n",
      "9/P_blockgroups_1/blocks_0_inner: 0.018289566040039062\n",
      "9/P_blockgroups_0/blocks_2_inner: 0.019852638244628906\n",
      "9/P_blockgroups_1/blocks_2_inner: 0.0\n",
      "9/P_bg1: 0.0\n",
      "9/P_blockgroups_2/blocks_0_inner: 0.11023139953613281\n",
      "10/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "10/P_blockgroups_2/blocks_2_inner: 0.2968482971191406\n",
      "10/P_blockgroups_1/blocks_2_inner: 0.0\n",
      "10/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "10/P_blockgroups_2/blocks_0_inner: 0.0\n",
      "10/P_blockgroups_2/blocks_1_inner: 0.21407699584960938\n",
      "10/P_bg1: 0.0\n",
      "10/P_blockgroups_1/blocks_0_inner: 0.0\n",
      "10/P_blockgroups_0/blocks_2_inner: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:04<00:31,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/P_bg2: 0.0\n",
      "10/P_bg0: 0.0\n",
      "10/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "11/P_blockgroups_2/blocks_1_inner: 0.0\n",
      "11/P_bg0: 0.0\n",
      "11/P_bg1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:04<00:35,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/P_bg2: 0.0\n",
      "11/P_blockgroups_1/blocks_1_inner: 0.0\n",
      "11/P_blockgroups_0/blocks_2_inner: 0.0\n",
      "11/P_blockgroups_2/blocks_2_inner: 0.0\n",
      "11/P_blockgroups_0/blocks_1_inner: 0.0\n",
      "11/P_blockgroups_0/blocks_0_inner: 0.0\n",
      "11/P_blockgroups_1/blocks_2_inner: 0.0\n",
      "11/P_blockgroups_1/blocks_0_inner: 0.0\n",
      "11/P_blockgroups_2/blocks_0_inner: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a_sd = model1_flax_sd # load_pickle(os.path.join(model_save_paths[0]))\n",
    "b_sd = model2_flax_sd # load_pickle(os.path.join(model_save_paths[1]))\n",
    "\n",
    "a_sd_union = flatten_params(a_sd['params'])\n",
    "a_sd_union.update(flatten_params(a_sd['batch_stats']))\n",
    "\n",
    "b_sd_union = flatten_params(b_sd['params'])\n",
    "b_sd_union.update(flatten_params(b_sd['batch_stats']))\n",
    "\n",
    "permutation_spec = resnet20_permutation_spec()\n",
    "\n",
    "final_permutation = weight_matching(\n",
    "    random.PRNGKey(0), permutation_spec,\n",
    "    # flatten_params(model_a), \n",
    "    # flatten_params(model_b)\n",
    "    a_sd_union, \n",
    "    b_sd_union\n",
    ")\n",
    "\n",
    "model_b_params_clever = unflatten_params(apply_permutation(permutation_spec, final_permutation, flatten_params(b_sd['params'])))\n",
    "model_b_stats_clever = unflatten_params(apply_permutation(permutation_spec, final_permutation, flatten_params(b_sd['batch_stats'])))\n",
    "clever_params_sd = lerp(.5, a_sd['params'], model_b_params_clever.unfreeze())\n",
    "clever_stats_sd = lerp(.5, a_sd['batch_stats'], model_b_stats_clever.unfreeze())\n",
    "clever_p_sd = {'params': clever_params_sd, 'batch_stats': clever_stats_sd}\n",
    "# train_ds, test_ds = load_cifar100()\n",
    "\n",
    "# model = ResNet(blocks_per_group=BLOCKS_PER_GROUP[\"resnet20\"], num_classes=512, width_multiplier=4)\n",
    "# stuff = make_stuff(model)\n",
    "# test_loss, test_acc1, test_acc5 = stuff[\"dataset_loss_and_accuracies\"](clever_p_sd, test_ds, 1000)\n",
    "# print('Acc: {}'.format(test_acc1))\n",
    "# save_path = os.path.join(model_dir, pair[0], 'flax_cifar50_2_permuted_to_1.pkl')\n",
    "# print(save_path)\n",
    "# pickle.dump(clever_p_sd, open(save_path, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Rebasin Model From Flax to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to pytorch from jax.\n",
    "def collapse_dict(jax_params_dict):\n",
    "    torch_params_dict = dict()\n",
    "    \n",
    "    for old_dict in [jax_params_dict]:\n",
    "        # print(old_dict)\n",
    "        recursively_build_dict([], old_dict, torch_params_dict)\n",
    "    # pprint(torch_params_dict)\n",
    "    torch_params_dict = {fix_keys(k): v for k, v in torch_params_dict.items()}\n",
    "    torch_params_dict = {k: fix_vals(k, v) for k, v in torch_params_dict.items()}\n",
    "\n",
    "    return torch_params_dict\n",
    "def recursively_build_dict(old_keys, old_dict, new_dict):\n",
    "    if isinstance(old_dict, flax.core.frozen_dict.FrozenDict):\n",
    "        for old_key, old_val in old_dict.items():\n",
    "            recursively_build_dict(old_keys + [old_key], old_val, new_dict)\n",
    "    else:\n",
    "        # now we have an array to convert\n",
    "        new_dict[\".\".join(old_keys)] = old_dict\n",
    "def fix_keys(old_key):\n",
    "    new_key = old_key\n",
    "    new_key = re.sub(r\"norm(\\d).scale\", \"norm\\g<1>.kernel\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.layers_\", \"shortcut.\", new_key)\n",
    "    new_key = re.sub(r\"shortcut\\.1\\.scale\", \"shortcut.1.weight\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\\.blocks_(\\d)\\.\", \"blockgroups_\\g<1>.\\g<2>.\", new_key)\n",
    "    new_key = re.sub(r\"blockgroups_(\\d)\", lambda x: f\"blockgroups_{int(x.group(1))+1}\", new_key)\n",
    "    substitutions =[(\"bn\", \"norm\"),(\"layer\", \"blockgroups_\"),(\"running_mean\", \"mean\"),(\"running_var\", \"var\"),(\"weight\", \"kernel\"),(\"linear\",\"dense\")]\n",
    "    for sub in substitutions[::-1]:\n",
    "        new_key = new_key.replace(sub[1], sub[0]) # in reverse order of old fix_keys\n",
    "    return new_key\n",
    "def fix_vals(old_key, old_val):\n",
    "    new_val = old_val\n",
    "    if \"conv\" in old_key or 'shortcut.0' in old_key:\n",
    "        # new_val = jnp.transpose(new_val, (2, 3, 1, 0))\n",
    "        new_val = jnp.transpose(new_val, (3, 2, 0, 1))\n",
    "    elif 'linear.weight' in old_key:\n",
    "        new_val = jnp.transpose(new_val, (1, 0))\n",
    "\n",
    "    new_val = torch.tensor(np.array(new_val))\n",
    "    \n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "output_dict = collapse_dict(flax.core.frozen_dict.FrozenDict(clever_p_sd))\n",
    "print(len(output_dict))\n",
    "\n",
    "save_dir = '/srv/share2/gstoica3/checkpoints/cifar50_traincliphead/gitrebasins'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f'{eval_pair}.pkl')\n",
    "pickle.dump(output_dict, open(save_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/share2/gstoica3/checkpoints/cifar50_traincliphead/gitrebasins/4.pkl'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 3, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['params.conv1.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-mmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
